{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 2 - Optimization, Backpropagation and Conclusion\n",
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Review\n",
    "\n",
    "In this 2$^{nd}$ part of Neural Networks we'll look at the optimization function that we'll be using for a Neural Network. We'll also look at Backpropagation, an algorithm that revolutionalized parameter learning in Neural Networks. After that we'll take a look at a modified Gradient Descent algorithm called Mini-batch Gradient Descent and then briefly look at multiclass classification. Finally we'll sum up Neural Networks and take a look at the workflow of a Neural Network model. So this is going to be a long notebook but you're going to learn a lot and by the end of this notebook you'll have successfully unmasked this black box called Neural Networks. So, before we dive into all these new concepts, let's take a brief look at Part 1 and note the important points to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We first saw the general architecture of a Neural Network involving input layer, hidden layers and ouput layers. Each layer consists of a set of neurons called units which is just a single logistic unit. <b>Remember that an $L$ layered Neural network has $(L-1)$ hidden layers and $1$ output layer. Input layer is the $0^{th}$ layer</b>.</li>\n",
    "    <li>\n",
    "        Then we looked at Forward Propagation. Although the equations were given for a 2 layer network, it can be generalized to L layers as follows.<br>\n",
    "        The forward propagation equations for the $l^{th}$ layer are given by,<br><br>\n",
    "        $$\\boxed{\\begin{align}\n",
    "        Z^{[l]} &= W^{[l]}.A^{[l-1]} + b^{[l]} \\\\\\\\\n",
    "        A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "        \\end{align}}$$\n",
    "        where, $g^{[l]}(.)$ is the activation function of $l^{th}$ layer.\n",
    "    </li>\n",
    "    <li> We then learnt initialization of parameters (<b>Randomly for weights and zeros for biases</b>). We also looked at the shapes of the parameter matrices.<br>\n",
    "        If the $[l-1]^{th}$ layer has $n^{[l-1]}$ units and $l^{th}$ layer has $n^{[l]}$ units then, <br>\n",
    "        <ul>\n",
    "            <li>$W^{[l]}$ has shape $(n^{[l]} \\times n^{[l-1]})$.</li>\n",
    "            <li>$b^{[l]}$ has shape $(n^{[l]} \\times 1)$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Later we looked at the different kinds of non-linear functions called <b>activation functions</b> like ReLU, Leaky ReLU, $\\tanh$ and saw their advantages and disadvantages.</li>\n",
    "    <li>Note that for a quantity X, <br>\n",
    "        $X^{(i)[l]}_j$ means that<br>\n",
    "        <ul>\n",
    "            <li>It is in the $[l]^{th}$ layer.</li>\n",
    "            <li>It is in the $j^{th}$ unit of that layer.</li>\n",
    "            <li>It is part of the $(i)^{th}$ training example.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "Now that we are done with the review let's dive right in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimization Function\n",
    "\n",
    "After randomly initializing weights and biases, you have performed a forward pass through the neural network and got an output , a value between 0 and 1. But how do you measure how good or how bad of an output this is? That’s where our optimization function steps in.<br>\n",
    "As we have already seen in Linear Regression and Logistic Regression, the optimization function is the function which we need to minimize or maximize. In our case we need to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. The Loss Function\n",
    "\n",
    "The loss function is a function to measure how well a model is performing on a given example. It takes the ground truth value of a variable ($y$) and the estimated value of that variable by the model ($\\hat y$) which intuitively represents some kind of “cost” associated with the event. Our goal is to minimize the loss function.<br><br>\n",
    "The loss function that we’ll be using to measure how good the neural network performs is one that you are already familiar with. The Log loss or Binary Cross Entropy. In fact this is the loss that most binary classifiers use. The Loss function for the $i^{th}$ training example is given by,<br>\n",
    "\n",
    "$$L(y, \\hat y) =  y \\log (\\hat y) + (1 - y) \\log (1 - \\hat y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. How does this loss function work\n",
    "\n",
    "In the loss function $y$ is the ground truth value that needs to be predicted and it takes on values 0 or 1 i.e, $y \\in \\{0, 1\\}$ and $\\hat y$ is obtained from forward propagation and takes on values between 0 and 1 i.e, $\\hat y \\in (0, 1)$. For a good classification model, $y$ and $\\hat y$ should be close to each other.\n",
    "<br><br>So,<br>\n",
    "If $y = 1$, $L(y, \\hat y) = - \\log (\\hat y)$. Therefore the closer $\\hat y$ gets to $1$, $\\log (\\hat y)$ becomes less and less negative and hence the loss gets closer to 0.<br><br>\n",
    "On the other hand, if $y = 0$, $L(y, \\hat y) = - \\log (1 - \\hat y)$. Therefore the closer $\\hat y$ gets to $0$, $\\log (1 - \\hat y)$ becomes less and less negative and hence the loss gets closer to 0.<br><br>\n",
    "Also, if by some extremely small chance, $y = 1$ and $\\hat y \\approx 0$ or $y = 0$ and $\\hat y \\approx 1$, $L(y, \\hat y) \\rightarrow \\infty$, which is what we would want for such a bad prediction.\n",
    "<br><br>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>:<br>\n",
    "    It might be helpful if you take a look at the log graph ($x \\in (0,1)$) to see how exactly this works out.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 The Cost Function\n",
    "\n",
    "Since our training set contains $m$ training examples, we define the cost function as the average of loss function over the m training examples. In fact this cost function is what we will be actually minimizing. The cost function is given by,<br><br>\n",
    "$$J(Y, \\hat Y) = -\\frac{1}{m} \\sum\\limits_{i = 1}^m y^{(i)} \\log (\\hat y^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat y^{(i)})$$\n",
    "<br>\n",
    "where, $y^{(i)}$ refers to the $i^{th}$ column of $Y$ i.e, ground truth value of $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Vectorization\n",
    "\n",
    "The above equation in vectorized format is given by,<br><br>\n",
    "$$\\boxed{J(Y, \\hat Y) = -\\frac{1}{m} * SUM(Y \\log (\\hat Y) + (1 - Y) \\log (1 - \\hat Y))}$$\n",
    "<br>\n",
    "Where,<br>\n",
    "<ul>\n",
    "    <li>$Y$ and $\\hat Y$ are $1 \\times m$ dimensional vectors.</li>\n",
    "    <li>$ SUM $ refers to summing over all elements of the matrix.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "So now that you have measured how good or bad your output is using the cost function, how do you give feedback to your network about this  cost? How do you adjust your weights and biases to get a better output? Enter Backpropagation - the most mathematically intense part of a Neural Network.<br><br>\n",
    "Backpropagation is a fancy name for the ‘chain rule’ which you would have definitely come across in your Calculus course (yup, MATH F111 it is). Eventhough backpropagation strictly refers to the computation of gradients of the loss function with respect to the different weights and biases in a Neural Network, it is generally used to refer to the entire learning algorithm of a Neural Network.<br><br>\n",
    "\n",
    "<center><img src = \"https://miro.medium.com/max/1400/1*YuotNxDwryjp3FiOwhVIkg.jpeg\" width = \"400\"></center>\n",
    "<br>\n",
    "<div class = \"alert alert-block alert-info\">\n",
    "<b>From here on, the Mathematics may seem complicated, but if you slowly go through it step by step, you can see that it's just simple calculus.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Overview\n",
    "\n",
    "In backpropagation, our main goal is to find the gradient of the cost function $J$ with respect to the parameters of an $L$ layered Neural Network $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \\ldots W^{[L]}, b^{[L]}$ i.e, $\\frac{\\partial J}{\\partial W^{[l]}}$ and $\\frac{\\partial J}{\\partial b^{[l]}}$ for $l = 1, 2, \\ldots L$. Once we can find these, we use it to update our parameters using our trusty algorithm Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Obtaining the Gradients\n",
    "\n",
    "Consider a 3 layered Neural Network as shown<br>\n",
    "<div class = \"alert alert-block alert-info\">\n",
    "    Note that in the image:<br>\n",
    "    <ul>\n",
    "        <li>$Z1, A1, X1, \\dots$ actually mean $Z_1, A_1, X_1 \\dots$</li>\n",
    "        <li>$W1, W2, b1, b2, \\dots$ actually mean $W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}, \\dots$</li>\n",
    "        <li>$L1, L2 \\dots$ refers to $Layer 1, Layer 2 \\dots$\n",
    "    </ul>\n",
    "</div>\n",
    "<img src = \"https://ik.imagekit.io/cpj5jrovil/3layer_v6uGpBW26.JPG\" width = \"800\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have and what we need.<br><br>\n",
    "\n",
    "What we have:<br>\n",
    "<b>Forward propagation equations in vectorized form</b><br>\n",
    "<ul>\n",
    "    <li>Layer 1: $Z^{[1]} = W^{[1]} . X + b^{[1]}$  and  $A^{[1]} = g^{[1]}(Z^{[1]})$</li>\n",
    "    <li>Layer 2: $Z^{[2]} = W^{[2]} . A^{[1]} + b^{[2]}$  and  $A^{[2]} = g^{[2]}(Z^{[2]})$</li>\n",
    "    <li>Layer 3: $Z^{[3]} = W^{[3]} . A^{[2]} + b^{[3]}$  and  $A^{[3]} = g^{[3]}(Z^{[3]}) = \\hat Y$</li>\n",
    "</ul><br>\n",
    "where for the $l^{th}$ layer,<br>\n",
    "<ul>\n",
    "    <li>$W^{[l]}$ is the weight matrix with dimension $n^{[l]} \\times n^{[l-1]}$.</li>\n",
    "    <li>$b^{[l]}$ is the bias vector with dimension $n^{[l]} \\times 1$.</li>\n",
    "    <li>$g^{[l]}(.)$ is the activation function (e.g, sigmoid, relu, etc.).</li>\n",
    "</ul>\n",
    "<br>\n",
    "<b>Cost function</b><br>\n",
    "$J(Y, \\hat Y) = -\\frac{1}{m} \\sum\\limits_{i = 1}^m y^{(i)} \\log (\\hat y^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat y^{(i)})$\n",
    "<br><br>\n",
    "What we need:<br>\n",
    "<b>Gradients of cost function with respect to parameters</b><br><br>\n",
    "$$\\frac{\\partial J}{\\partial W^{[3]}}, \\frac{\\partial J}{\\partial b^{[3]}}, \\frac{\\partial J}{\\partial W^{[2]}}, \\frac{\\partial J}{\\partial b^{[2]}}, \\frac{\\partial J}{\\partial W^{[1]}}, \\frac{\\partial J}{\\partial b^{[1]}}$$\n",
    "<br><br>\n",
    "These equations can be represented by the following graph<br>\n",
    "<center><img src = \"https://ik.imagekit.io/cpj5jrovil/Forward_flow_KjdPF_OZJ.jpeg\" width = \"350\"></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the above graph, we can get the gradients in the $3^{rd}$ layer as follows using chain rule:<br><br>\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w^{[3]}} & = \\frac{\\partial J}{\\partial a^{[3]}}.\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial w^{[3]}} \\\\\\\\\n",
    "\\frac{\\partial J}{\\partial b^{[3]}} & = \\frac{\\partial J}{\\partial a^{[3]}}.\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial b^{[3]}}\n",
    "\\end{align}$$\n",
    "<br><br>\n",
    "For the $2^{nd}$ layer two more terms get added:<br><br>\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w^{[2]}} & = \\frac{\\partial J}{\\partial a^{[3]}}.\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}.\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}.\\frac{\\partial z^{[2]}}{\\partial w^{[2]}} \\\\\\\\\n",
    "\\frac{\\partial J}{\\partial b^{[2]}} & = \\frac{\\partial J}{\\partial a^{[3]}}.\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}.\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}.\\frac{\\partial z^{[2]}}{\\partial b^{[2]}}\n",
    "\\end{align}$$\n",
    "<br><br>\n",
    "Similarly for the $1^{st}$ layer two more terms will get added.\n",
    "<br><br>\n",
    "These equations look very complicated. So we can use a computational graph like this to see how exactly we propagate backwards and what gradients we get in each step. Try comparing it with the above equations<br>\n",
    "<center><img src = \"https://ik.imagekit.io/cpj5jrovil/Backward_flow_731lDVkWT.jpeg\" width = \"250\"></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the required gradients one by one. For now, we'll compute all the gradients for a single training example and generalize later on.<br><br>\n",
    "<ol>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial a^{[3]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\because \\hat y = a^{[3]}, \\qquad J &= - (y \\log (a^{[3]}) + (1 - y) \\log (1 - a^{[3]})) \\\\\\\\\n",
    "        \\implies \\frac{\\partial J}{\\partial a^{[3]}} &= - \\left(\\frac{y}{a^{[3]}} - \\frac{1-y}{1 - a^{[3]}}\\right) \\tag{1}\n",
    "        \\end{align}$$\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial z^{[3]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial z^{[3]}} &= \\frac{\\partial J}{\\partial a^{[3]}}.\\frac{\\partial a^{[3]}}{\\partial z^{[3]}} \\\\\\\\\n",
    "        \\because a^{[3]} = g^{[3]}(z^{[3]}), \\qquad \\frac{\\partial J}{\\partial z^{[3]}} &= \\underbrace{\\frac{\\partial J}{\\partial a^{[3]}}}_{\\text{(1)}}*g^{[3] \\prime}(z^{[3]}) \\tag{2}\n",
    "        \\end{align}$$<br>\n",
    "        Notice that we use derivative of the activation function in this equation (which you would have seen in Neural Networks Part 1).\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial w^{[3]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial w^{[3]}} &= \\frac{\\partial J}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial w^{[3]}} \\\\\\\\\n",
    "        \\because z^{[3]} = w^{[3]}.a^{[2]} + b^{[3]}, \\qquad \\frac{\\partial J}{\\partial w^{[3]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[3]}}}_{\\text{(2)}}.a^{[2]} \\tag{3}\n",
    "        \\end{align}$$\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial b^{[3]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial b^{[3]}} &= \\frac{\\partial J}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial b^{[3]}} \\\\\\\\\n",
    "        \\because z^{[3]} = w^{[3]}.a^{[2]} + b^{[3]}, \\qquad \\frac{\\partial J}{\\partial b^{[3]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[3]}}}_{\\text{(2)}}.(1) \\tag{4}\n",
    "        \\end{align}$$\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial a^{[2]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial a^{[2]}} &= \\frac{\\partial J}{\\partial z^{[3]}}.\\frac{\\partial z^{[3]}}{\\partial a^{[2]}} \\\\\\\\\n",
    "        \\because z^{[3]} = w^{[3]}.a^{[2]} + b^{[3]}, \\qquad \\frac{\\partial J}{\\partial a^{[2]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[3]}}}_{\\text{(2)}}.w^{[3]} \\tag{5}\n",
    "        \\end{align}$$\n",
    "    </li><br>\n",
    "    This completes the computation for gradients involving 3rd layer. Let's compute some more gradients to see if we can notice any patterns.<br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial z^{[2]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial z^{[2]}} &= \\frac{\\partial J}{\\partial a^{[2]}}.\\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\\\\\\\\n",
    "        \\because a^{[2]} = g^{[2]}(z^{[2]}), \\qquad \\frac{\\partial J}{\\partial z^{[2]}} &= \\underbrace{\\frac{\\partial J}{\\partial a^{[2]}}}_{\\text{(5)}}*g^{[2] \\prime}(z^{[2]}) \\tag{6}\n",
    "        \\end{align}$$<br><br>\n",
    "        Compare this equation with (2)\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial w^{[2]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial w^{[2]}} &= \\frac{\\partial J}{\\partial z^{[2]}}.\\frac{\\partial z^{[2]}}{\\partial w^{[2]}} \\\\\\\\\n",
    "        \\because z^{[2]} = w^{[2]}.a^{[1]} + b^{[2]}, \\qquad \\frac{\\partial J}{\\partial w^{[2]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[2]}}}_{\\text{(6)}}.a^{[1]} \\tag{7}\n",
    "        \\end{align}$$\n",
    "        Compare this equation with (3)\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial b^{[2]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial b^{[2]}} &= \\frac{\\partial J}{\\partial z^{[2]}}.\\frac{\\partial z^{[2]}}{\\partial b^{[2]}} \\\\\\\\\n",
    "        \\because z^{[2]} = w^{[2]}.a^{[1]} + b^{[2]}, \\qquad \\frac{\\partial J}{\\partial b^{[2]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[2]}}}_{\\text{(6)}}.(1) \\tag{8}\n",
    "        \\end{align}$$\n",
    "        Compare this equation with (4)\n",
    "    </li><br><br>\n",
    "    <li>\n",
    "        $\\frac{\\partial J}{\\partial a^{[1]}}$:<br>\n",
    "        $$\\begin{align}\n",
    "        \\frac{\\partial J}{\\partial a^{[1]}} &= \\frac{\\partial J}{\\partial z^{[2]}}.\\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\\\\\\\\n",
    "        \\because z^{[2]} = w^{[2]}.a^{[1]} + b^{[2]}, \\qquad \\frac{\\partial J}{\\partial a^{[1]}} &= \\underbrace{\\frac{\\partial J}{\\partial z^{[2]}}}_{\\text{(6)}}.w^{[2]} \\tag{9}\n",
    "        \\end{align}$$\n",
    "        Compare this equation with (5)\n",
    "    </li>\n",
    "</ol>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would have compared equations (6), (7), (8) and (9) with (2), (3), (4) and (5) respectively, you definitely would have noticed the similarity. In fact if you derive the equations for one more layer, the pattern repeats.<br>\n",
    "From this we can infer the <b>four equations for backpropagation</b> as follows:\n",
    "\n",
    "For any layer $l$ in a Neural Network,<br>\n",
    "$$\\begin{align}\n",
    "1.& \\; \\frac{\\partial J}{\\partial z^{[l]}} = \\frac{\\partial J}{\\partial a^{[l]}}*g^{[l] \\prime}(z^{[l]}) \\\\\\\\\n",
    "2.& \\; \\frac{\\partial J}{\\partial w^{[l]}} = \\frac{\\partial J}{\\partial z^{[l]}}.a^{[l-1]} \\\\\\\\\n",
    "3.& \\; \\frac{\\partial J}{\\partial b^{[l]}} = \\frac{\\partial J}{\\partial z^{[l]}} \\\\\\\\\n",
    "4.& \\; \\frac{\\partial J}{\\partial a^{[l-1]}} = \\frac{\\partial J}{\\partial z^{[l]}}.w^{[l]}\n",
    "\\end{align}$$<br>\n",
    "By applying these four equation for layers in the order $L, (L-1), \\ldots 2, 1$ in an $L$ layered Neural Network you would have successfully implemented backpropagation.<br>\n",
    "However there is one step that doesn't come under this pattern. Computation of $\\frac{\\partial J}{\\partial a^{[L]}}$ where $L$ is the last layer. This needs to be computed first and separately as it involves directly differentiating the cost function. This can be seen in equation (1).<br><br>\n",
    "<div class=\"alert alert-block alert-info\"><b>\n",
    "Note that there is no need to remember the entire math behind backpropagation. These four equations are what we'll be actually using. The math was given just for clarity and better understanding.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase computation speed we have already vectorized forward propagation and cost calculation. Likewise, let's vectorize backpropagation.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Notation information:</b>:<br>\n",
    "    <ul>\n",
    "        <li>\n",
    "    From now on, to represent gradients we'll be using slightly different notations which although technically inaccurate will make writing and coding easier.<br>\n",
    "    If we were to calculate the gradient of cost function $J$ with respect to a parameter $\\theta$ i.e, $\\frac{\\partial J}{\\partial \\theta}$, we will represent it as $d \\theta$.\n",
    "    So $\\frac{\\partial J}{\\partial w^{[l]}}$ will be represented as $dw^{[l]}$, $\\frac{\\partial J}{\\partial A^{[l]}}$ will be represented as $dA^{[l]}$ and so on.<br>\n",
    "        </li>\n",
    "    <li>Uppercase character represents matrix form of the corresponding lowercase character. For example, $W^{[l]}$ is the matrix form of $w^{[l]}$.</li>\n",
    "        <li>Also remember that in matrix form, <b>$\\theta$ and $d\\theta$ will have same shape</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an L layered Neural Network (binary classification):\n",
    "<ul>\n",
    "    <li>$dA^{[L]}$ can be compuuted as:<br><br>\n",
    "        $$\\boxed{dA^{[L]} = - \\left( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1 - A^{[L]}} \\right)}$$\n",
    "    </li><br>\n",
    "    <li>The four equations of backpropagation for the $l^{th}$ are given by:<br><br>\n",
    "        $$\\boxed{\\begin{align}\n",
    "        1.& \\, dZ^{[l]} = dA^{[l]} * g^{[l] \\prime}(z^{[l]}) \\\\\\\\\n",
    "        2.& \\, dW^{[l]} = \\frac{1}{m}dZ^{[l]}.A^{[l-1]^T} \\\\\\\\\n",
    "        3.& \\, db^{[l]} = \\frac{1}{m} \\sum _{col} dZ^{[l]} \\\\\\\\\n",
    "        4.& \\, dA^{[l-1]} = W^{[l]^T}.dZ^{[l]}\n",
    "        \\end{align}}$$<br>\n",
    "        where,<br>\n",
    "        <ul>\n",
    "            <li>$\\sum \\limits_{col}$ refers to <b>summing across columns</b>.</li>\n",
    "            <li>$m$ is the number of training examples.</li>\n",
    "        </ul>\n",
    "</ul>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Some important point to note:</b>\n",
    "    <ul>\n",
    "        <li>Notice how the formula changes slightly for matrix form from elementwise form.</li>\n",
    "        <li>Don't forget that we use the derivative of the activation function in equation 1</li>\n",
    "        <li>'$.$' refers to matrix multiplication while '$*$' refers to elementwise multiplication.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the gradients for parameters of all layers have been calculated we proceed to update the parameters using Gradient Descent like we did for Linear and Logistic Regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform gradient descent for $T$ iterations for a Neural Network of $L$ layers:<br><br>\n",
    "<center>\n",
    "$\\boxed{\\begin{align}\n",
    "for \\; &t = 1 \\ldots T: \\\\\\\\\n",
    "&for \\; l = 1 \\ldots L: \\\\\\\\\n",
    "&\\qquad W^{[l]} = W^{[l]} - \\alpha*dW^{[l]} \\\\\\\\\n",
    "&\\quad \\quad \\ \\ \\ b^{[l]} = b^{[l]} - \\alpha*dW^{[l]}\n",
    "\\end{align}}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini-batch Gradient Descent\n",
    "\n",
    "While working with Neural Networks, we generally deal with large amount of data. Suppose you are dealing with a dataset having 500,000 training examples. Then during forward propagation, how many examples do you need to process to perform a single step of gradient descent? All 500,000 of them, since we are taking it as a single matrix. As the Neural Network becomes deeper (i.e, th nuumber of layers increase) the computation becomes more and more expensive. So mini-batch gradient descent divides the training set into smaller 'mini-batches' and processes one mini-batch for a single gradient descent step. Let's look into this in a little more detail.<br>\n",
    "\n",
    "Consider our training set with 500,000 examples. Now let's divide this training set into mini-batches of 1000 examples. To do this we first randomly shuffle the training data i.e, columns in $X$ and <b>corresponding</b> columns in $Y$. Once this is done, we take the first 1000 examples i.e, the first 1000 columns of $X$ and <b>corresponding</b> columns in $Y$ (remember that in $X$ and $Y$ columns refer to training examples) and call it mini-batch $1$ i.e, $( X^{\\{1\\}}, Y^{\\{1\\}})$. We take the next 1000 examples similarly and call it mini-batch $2$ i.e, $( X^{\\{2\\}}, Y^{\\{2\\}})$ and so on. In the end, for 500,000 examples we'll have 500 mini-batches with 1000 examples each.<br>\n",
    "\n",
    "Now we take each mini-batch instead of the entire training set and perform one step of gradient descent on it. So for example, our forward propagation equation which was $Z^{[1]} = W^{[1]}.X + b^{[1]}$ will become, $Z^{[1]} = W^{[1]}.X^{\\{t\\}} + b^{[1]}$ for the $t^{th}$ mini-batch. We also use this mini-batch to perform one step of gradient descent. So by the time we use the entire training set once, we would have performed 500 steps of gradient descent, where on the other hand in the previous algorithm one pass through the training set would perform a single step of gradient descent.<br>\n",
    "\n",
    "One pass through the entire training set i.e, one iteration over all the mini-batches is called 1 <b>epoch</b>. So we can have multiple epochs to perform multiple passes through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a side by side comparison of the two methods.\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Gradient Descent</th>\n",
    "        <th>Mini-batch Gradient Descent</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            $$\\begin{align}\n",
    "            for \\; &i = 1 \\ldots num\\_iterations: \\\\\\\\\n",
    "            & \\{Gradient \\, Descent \\, Equations \\, for \\, X \\, and \\, Y\\}\n",
    "            \\end{align}$$\n",
    "        </td>\n",
    "        <td>\n",
    "            $$\\begin{align}\n",
    "            for \\; &i = 1 \\ldots num\\_epochs: \\\\\\\\\n",
    "            &for \\; t = 1 \\ldots num\\_minibatches: \\\\\\\\\n",
    "            & \\qquad \\{Gradient \\, Descent \\, Equations \\, for \\, X^{\\{t\\}} \\, and \\, Y^{\\{t\\}} \\}\n",
    "            \\end{align}$$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why this algorithm works better for large datasets and how it differs from the one we have been seeing so far, make sure you check out the videos linked in the Additional Resources section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiclass Classification\n",
    "\n",
    "#### 4.1. Introduction\n",
    "\n",
    "So far we've looked at a Neural Network which can perform binary classifications i.e, 0 or 1. However many times we need to deal with multiclass classification tasks like, recognizing handwritten digits (0-9), Iris data (classify into 3 flowers), etc. It's very simple to transition from binary classification to multiclass classification. The only difference is in the activation function of the output layer. While we use the <b>sigmoid</b> activation in binary classification, we'll use the <b>softmax</b> activation function for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. The Softmax Activation\n",
    "\n",
    "The softmax activation function is given by, <br>\n",
    "$$\\boxed{S(z_i) = \\frac{e^{z_i}}{\\sum\\limits_{j=1}^{n^{[L]}} e^{z_j}}}$$\n",
    "<br><br>\n",
    "In the ouput layer you will have computed $Z^{[L]}$, an $(n^{[L]} \\times 1)$ vector. The softmax function takes this entire vector as input and ouputs another vector $A^{[L]}$ of same dimensions where each entry is a number between 0 and 1. Also note that sum off all values in $A^{[L]}$ will be equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Cost function\n",
    "\n",
    "Since our activation function changes at the output layer we will need to modify the Cost function. The new cost function for the multiclass classifier is similar to the old one and is called <b>Categorical Cross Entropy</b> and defined by,<br><br>\n",
    "$$\\boxed{J(Y, \\hat Y) = - \\frac{1}{m} SUM(Y \\log (\\hat Y))}$$\n",
    "<br>\n",
    "Note that here $Y$ and $\\hat Y = A^{[L]}$ will be $(n^{[l]} \\times m)$ shaped matrices in their one hot representations. Also since the cost functions change, the first step of backpropagation will also face minor changes. Try to figure out those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>:\n",
    "    There is also one more way to perform multiclass classification. It is called the <b>One vs All</b> classifier. If we have 10 classes, we train 10 binary classifiers where each one classifies one  particular class . Try finding out how exactly this method works.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "So now that we have looked at Neural Networks in detail, let's note all the important steps to remember. This will help you fit all the pieces together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Neural Network Workflow (for a Binary Classifier)\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>Defining the Neural Network Architecture</b>: Define the structure of your Neural Network by chosing the number of layers, number of units in each layer and the activation function of each layer.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Initializing parameters</b>: Initialize your Weight matrices randomly and Bias vectors to zeros. Make sure your matrices and vectors are in their <b>correct shapes</b>.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Mini-batch creation</b>: Divide your traing data $X_{train}$ and $Y_{train}$ into proper mini-batches.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Training</b>:<br>\n",
    "        Now you start your training process. Iterate for a certain number of <b>epochs</b> and for each epoch iterate through all the mini-batches. For each mini-batch perform the following steps:<br>\n",
    "        <ul>\n",
    "            <li><b>Forward Propagation</b>: Propagate through the Neural network for layers $l = 1 \\ldots L$. The equations for the $l^{th}$ layer are:<br><br>\n",
    "                $$\\boxed{\\begin{align}\n",
    "                Z^{[l]} &= W^{[l]}.A^{[l-1]} + b^{[l]} \\\\\\\\\n",
    "                A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "                \\end{align}}$$<br>\n",
    "            </li>\n",
    "            <li><b>Compute the Cost</b>: Compute the cost for this iteration using,<br><br>\n",
    "                $$\\boxed{J(Y, \\hat Y) = -\\frac{1}{m} * SUM(Y \\log (\\hat Y) + (1 - Y) \\log (1 - \\hat Y))}$$<br>\n",
    "                where, $\\hat Y = A^{[L]}$\n",
    "            </li>\n",
    "            <li>\n",
    "                <b>Backpropagation</b>:<br>\n",
    "                <ul>\n",
    "                    <li><b>Compute gradient at output layer</b> using,<br><br>\n",
    "                        $$\\boxed{dA^{[L]} = - \\left( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1 - A^{[L]}} \\right)}$$<br>\n",
    "                    </li>\n",
    "                    <li><b>Apply the four equations of backrpopagation</b> to compute gradients through layers $l = L \\ldots 1$. The equations for $l^{th}$ layer are,<br><br>\n",
    "                         $$\\boxed{\\begin{align}\n",
    "                            1.& \\, dZ^{[l]} = dA^{[l]} * g^{[l] \\prime}(z^{[l]}) \\\\\\\\\n",
    "                            2.& \\, dW^{[l]} = \\frac{1}{m}dZ^{[l]}.A^{[l-1]^T} \\\\\\\\\n",
    "                            3.& \\, db^{[l]} = \\frac{1}{m} \\sum _{col} dZ^{[l]} \\\\\\\\\n",
    "                            4.& \\, dA^{[l-1]} = W^{[l]^T}.dZ^{[l]}\n",
    "                            \\end{align}}$$<br>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><b>Update the parameters</b>: Using the gradients computed through backpropagation, update the Weights and Biases for layers $l = 1 \\ldots L$ using,<br><br>\n",
    "                $$\\boxed{\\begin{align}\n",
    "                W^{[l]} & = W^{[l]} - \\alpha * dW^{[l]} \\\\\n",
    "                b^{[l]} & = b^{[l]} - \\alpha * db^{[l]}\n",
    "                \\end{align}}$$<br>\n",
    "                where, $\\alpha$ is the learning rate.\n",
    "            </li>\n",
    "        </ul>\n",
    "        This completes the training process.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Evaluation</b>: Evaluate your model using $X_{test}$ and $Y_{test}$. Remember that we want our model to generalize to unseen data. So it is the $test$ set accuracy that actually matters.\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<b>By implementing these 5 steps correctly you will have successfully implemented an Articial Neural Network.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Summary of notations and shapes\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <h5><b>Notations</b></h5>\n",
    "        <ol>\n",
    "            <li>\n",
    "                $X^{(i)\\{t\\}[l]}_j$ says that the quantity $X$,<br>\n",
    "                <ul>\n",
    "                    <li>Belongs to the $[l]^{th}$ layer.</li>\n",
    "                    <li>Belongs to the $\\{t\\}^{th}$ mini-batch.</li>\n",
    "                    <li>Is part of the $(i)^{th}$ training example.</li>\n",
    "                    <li>Is located at the $j^{th}$ unit of that layer.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>$n^{[l]}$ denotes the number of units in the $l^{th}$ layer.</li>\n",
    "            <li>$m$ denotes the number of training examples.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <h5><b>Shapes</b></h5>\n",
    "        <center>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Quantity</th>\n",
    "                    <th>Shape</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$X$$</td>\n",
    "                    <td>$$n_x \\times m$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$Y (binary)$$</td>\n",
    "                    <td>$$1 \\times m$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$Y (multiclass)$$</td>\n",
    "                    <td>$$n^{[L]} \\times m$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$Z{[l]}$$</td>\n",
    "                    <td>$$n^{[l]} \\times m$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$A{[l]}$$</td>\n",
    "                    <td>$$n^{[l]} \\times m$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$W^{[l]}$$</td>\n",
    "                    <td>$$n^{[l]} \\times n^{[l-1]}$$</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>$$b^{[l]}$$</td>\n",
    "                    <td>$$n^{[l]} \\times 1$$</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </center>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Hyperparameters\n",
    "\n",
    "While the Weights and Biases are <b>parameters</b> that your model learns on its own, there are some parameters which it can't learn. These parameters are called <b>hyperparameters</b> and are manually set by the ML engineer. You've already come across many hyperparameters. The number of layers, The number of units in each layer, Activation functions, Learning rate, Mini-batch size, Number of epochs are all hyperparameters. By tuning the hyperparameters, you can improve the performance of the model. In fact, if you've read the \"Introduction to ML\" article, you would have seen a step called \"Cross-Validation\". That step is generally used for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Deep Learning\n",
    "\n",
    "Deep Learning is a sub-field of Machine Learning that deals with Supervised and Unsupervised Learning using Networks. Artificial Neural Networks (ANNs) which you have just learnt is the first step in Deep Learning. Other Neural Networks include, Convolutional Neural Networks (CNNs) which are used for tasks like image recognition; Recurrent Neural Networks (RNNs) which are used for Sequence Models like Speech Recognition, Time Series Analysis, Natural Language Processing etc. Deep Learning is a vast field and has lots of applications in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "<ul>\n",
    "    <li>For an amazing intuition and clarity of how Neural Networks work, check out <a href = \"https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\">this</a> playlist by 3 Blue 1 Brown. Even though it takes around 1 hour, it's definitely worth it.</li>\n",
    "    <li>Needless to say, Andrew Ng's Deep Learning Specialization is one of the best, if you want to go deeper into this field of Machine Learning. <a href = \"https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&fbclid=IwAR2W10I3Enkrp15J7Eb2w-X-F9BwvVnwgGObAGx-Swz7SEektkiPMlDzufw\">Here</a> is the playlist dealing with ANNs.</li>\n",
    "    <li>For a better understanding of Backpropagation check <a href = \"https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536\">this</a> article.</li>\n",
    "    <li>Mini-batch Gradient Descent: <a href = \"https://youtu.be/4qJaSmvhxi8\">Part 1</a> and <a href = \"https://youtu.be/-_4Zi8fCZO4\">Part 2</a>.</li>\n",
    "    <li>Softmax: <a href = \"https://youtu.be/LLux1SW--oM\">Part 1</a> and <a href = \"https://youtu.be/ueO_Ph0Pyqk\">Part 2</a>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<ul>\n",
    "    <li>The notations used in this notebook are greatly influenced by Andrew Ng's Deep Learning Specialization.</li>\n",
    "    <li><a href = \"https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536\">This</a> Medium Article was used as reference for Backpropagation. The Forward popagation computation graph, Backward propagation computation graph and the meme were taken from the same article.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
