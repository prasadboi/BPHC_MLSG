{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-J9dvmz-JbK"
   },
   "source": [
    "# Deep Learning Frameworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0btGdz3-Ptr"
   },
   "source": [
    "Great job on completing the neural network assignment! Well, we know the pain of working with neural networks from scratch. Don't fret! - because there are multiple frameworks, like Tensorflow, Keras, Pytorch, Caffe, etc. out there to help you build a deep learning model easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCxc_AeBAKDn"
   },
   "source": [
    "# Why Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLxkBRcMANHF"
   },
   "source": [
    "Keras is an open source deep learning framework for python. It is a high level API that is built upon Tensorflow. Keras makes deployment of neural networks really simple, with just a few lines of code! The syntax is clear, simple and intuitive.  We urge you to go through the documentation after this to get a better feel of Keras. The link is: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GRWp2rrp2AK"
   },
   "source": [
    "#Importing Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K25nLJeRIFSL"
   },
   "source": [
    "We will first import tensorflow. Note that Keras uses Tensorflow 2.  as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_U64Pp7IOae"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d1e6d42ad48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\PRASAD\\anaconda3\\envs\\deepLearning1\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sq6aaZ0dEJTL"
   },
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTLfzHLFENcN"
   },
   "source": [
    "We will geenerate a random n-class classification problem using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "in3X10QhEYM1",
    "outputId": "0a5e9ab3-8374-41ce-991d-92a6fd3edd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10) (5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "#X, y = make_classification(n_samples=3000, n_informative = 4, n_features=10, n_classes=5, random_state=1)\n",
    "\n",
    "X,y =make_classification(n_samples=5000, n_informative = 8, n_features=10, n_redundant = 0, n_classes=5, random_state=1)\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Wr9zs4cFjac"
   },
   "source": [
    "We have created a dataset with m=1000 and n=10. Each training example belongs to one of 5 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1bX3RwKFuOJ"
   },
   "source": [
    "# Split and standardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RO8Os900Fycy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)   #Split data into 70% training, 30%test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2Xr0fLiGHh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    #Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfJImjfMZmzO"
   },
   "source": [
    "# Converting the labels to one hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjeXHOMcZrM1"
   },
   "source": [
    "To convert our targets into a one hot encoding we use the Keras to_categorical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dtx2XYc_Z39t"
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYyCI_EFHJo0"
   },
   "source": [
    "# **MAKING THE NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r3ox244Thad"
   },
   "source": [
    "##The Sequential class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJx4JnoqHgEy"
   },
   "source": [
    "The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. We assign it to a variable model.  Click [here](https://keras.io/api/models/sequential/) to check out the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X836dVLpIVB4"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N93HlSc1T1Jp"
   },
   "source": [
    "##The Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Aa-YdXEJJio"
   },
   "source": [
    "First, we call the Input() object of Keras, to instantiate a Keras tensor. Note that we only input the number of features and not the training size. So the shape argument of the Input() object is inputs (n, None) \n",
    "\n",
    "To add a certain object or layer to the Sequential model, we use the *add* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGWqM_HiJrRX"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.Input(shape=10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2gEqpjJUI2X"
   },
   "source": [
    "##Adding layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELKmipHsKUTo"
   },
   "source": [
    "To add a layer to a neural network, we add a dense layer, or the Dense class. Arguments to the Dense class include the number of neurons for the layer, the activation, regularizer and and other stuff which we do not need to worry about. \n",
    "\n",
    "In this model we will make an NN with 2 hidden layers, each activated by the relu function. The last layer , which is the output layer, has 5 neurons(one belonging to each class), activated by the softmax function. \n",
    "\n",
    "[Dense layer documentation link](https://keras.io/api/layers/core_layers/dense/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJBbxgEALDsT"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=30, activation='relu', )) #First hidden layer with 30 neurons, relu activation\n",
    "model.add(tf.keras.layers.Dense(units=15, activation='relu')) #Second hidden layer with 15 neurons, relu activation\n",
    "model.add(tf.keras.layers.Dense(units=5, activation='softmax')) #Output layer with 5 neuron, softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGnQ4HLEWm3M"
   },
   "source": [
    "##Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rli-9UekVftL"
   },
   "source": [
    "Note: Here we pass the type of activation as an argument to the Dense class. This is the same as adding an Activation layer with the activation inside it. So the first line of code above is the same as:\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation(tf.keras.activations.relu))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr8wmFs1W8FG"
   },
   "source": [
    "Click [here](https://keras.io/api/layers/activations/) to check out the various activations provided by keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDJy4tpfXIZi"
   },
   "source": [
    "##Model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNv3zOtWLjvO"
   },
   "source": [
    "Just that many lines of code to prototype our neural network model! Simple right? We can check out the model's details by calling the summary() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "4T6Pa9FbLuBU",
    "outputId": "ebfd9db4-171b-4bb7-feb1-c0f573111b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsMp9jtxXgFB"
   },
   "source": [
    "## The compile() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLmun9IpL7Ip"
   },
   "source": [
    "Next, we compile the model using the compile() method. Here we mention what our loss function should be, the type of optimizer used, and what metrics to print.\n",
    "\n",
    "We will be using the categorical cross-entropy as our loss function, with the adam optimizer and accuracy as our metric. Note that we pass our metrics as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMZ7TEqTMPns"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzogFygcX0ED"
   },
   "source": [
    "## Keras Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaQ6H21PX3Dz"
   },
   "source": [
    "Passing optimizer='adam' to the compile method uses the default learning rate and some other parameters. But often , we want to choose the learning rate, momentum, etc. So, we call the optimizer's class, store it in a variable and pass it as an argument to the compile method.\n",
    "\n",
    "For example, if we want to use stochastic gradient descent with a learning rate of 0.1 and momentum 0.9 , we would code it as follows:\n",
    "\n",
    "\n",
    "```\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer =opt, metrics=['accuracy'])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBmwz-qBZOV_"
   },
   "source": [
    "Click [here](https://keras.io/api/optimizers/) to check out the various Keras optimizers and their syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kyDJpKAZY3a"
   },
   "source": [
    "## The fit() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2zNL_SGNL2V"
   },
   "source": [
    "Finally , we call the fit() function to fit our data! Keras takes care of all the mathematical computation of forward prop and backprop! The fit() function takes into input the data, true labels, batch size in which we train, the number of epochs, and the validation data(here which is our test set). It returns a Keras history object which contains various attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Yx8qroZnNxjc",
    "outputId": "ca52c3a0-3b7f-42e4-bbd5-2aad61bb75f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.4770 - accuracy: 0.4046 - val_loss: 1.2602 - val_accuracy: 0.5413\n",
      "Epoch 2/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.0564 - accuracy: 0.6189 - val_loss: 0.9710 - val_accuracy: 0.6340\n",
      "Epoch 3/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8740 - accuracy: 0.6657 - val_loss: 0.8848 - val_accuracy: 0.6740\n",
      "Epoch 4/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7867 - accuracy: 0.7014 - val_loss: 0.8113 - val_accuracy: 0.7040\n",
      "Epoch 5/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7186 - accuracy: 0.7383 - val_loss: 0.7561 - val_accuracy: 0.7373\n",
      "Epoch 6/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.7597 - val_loss: 0.7119 - val_accuracy: 0.7473\n",
      "Epoch 7/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.7760 - val_loss: 0.6715 - val_accuracy: 0.7633\n",
      "Epoch 8/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.7891 - val_loss: 0.6369 - val_accuracy: 0.7773\n",
      "Epoch 9/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.8040 - val_loss: 0.6221 - val_accuracy: 0.7753\n",
      "Epoch 10/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.8117 - val_loss: 0.5932 - val_accuracy: 0.7947\n",
      "Epoch 11/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.8229 - val_loss: 0.5776 - val_accuracy: 0.7987\n",
      "Epoch 12/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.8263 - val_loss: 0.5760 - val_accuracy: 0.7920\n",
      "Epoch 13/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.8303 - val_loss: 0.5549 - val_accuracy: 0.7993\n",
      "Epoch 14/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.8351 - val_loss: 0.5439 - val_accuracy: 0.8073\n",
      "Epoch 15/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.8411 - val_loss: 0.5307 - val_accuracy: 0.8120\n",
      "Epoch 16/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8463 - val_loss: 0.5290 - val_accuracy: 0.8073\n",
      "Epoch 17/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.8480 - val_loss: 0.5132 - val_accuracy: 0.8180\n",
      "Epoch 18/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.8486 - val_loss: 0.5161 - val_accuracy: 0.8120\n",
      "Epoch 19/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8526 - val_loss: 0.5027 - val_accuracy: 0.8200\n",
      "Epoch 20/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8549 - val_loss: 0.4990 - val_accuracy: 0.8173\n",
      "Epoch 21/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.8543 - val_loss: 0.4971 - val_accuracy: 0.8273\n",
      "Epoch 22/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8597 - val_loss: 0.4994 - val_accuracy: 0.8233\n",
      "Epoch 23/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8560 - val_loss: 0.4915 - val_accuracy: 0.8220\n",
      "Epoch 24/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8654 - val_loss: 0.4947 - val_accuracy: 0.8253\n",
      "Epoch 25/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8634 - val_loss: 0.4880 - val_accuracy: 0.8273\n",
      "Epoch 26/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8614 - val_loss: 0.4819 - val_accuracy: 0.8260\n",
      "Epoch 27/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8689 - val_loss: 0.4750 - val_accuracy: 0.8320\n",
      "Epoch 28/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8680 - val_loss: 0.4859 - val_accuracy: 0.8280\n",
      "Epoch 29/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8669 - val_loss: 0.4735 - val_accuracy: 0.8307\n",
      "Epoch 30/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8709 - val_loss: 0.4729 - val_accuracy: 0.8373\n",
      "Epoch 31/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8720 - val_loss: 0.4693 - val_accuracy: 0.8353\n",
      "Epoch 32/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8726 - val_loss: 0.4626 - val_accuracy: 0.8393\n",
      "Epoch 33/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8763 - val_loss: 0.4611 - val_accuracy: 0.8360\n",
      "Epoch 34/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.8766 - val_loss: 0.4677 - val_accuracy: 0.8420\n",
      "Epoch 35/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8803 - val_loss: 0.4647 - val_accuracy: 0.8380\n",
      "Epoch 36/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8783 - val_loss: 0.4688 - val_accuracy: 0.8380\n",
      "Epoch 37/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8811 - val_loss: 0.4625 - val_accuracy: 0.8360\n",
      "Epoch 38/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8834 - val_loss: 0.4620 - val_accuracy: 0.8393\n",
      "Epoch 39/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8840 - val_loss: 0.4604 - val_accuracy: 0.8433\n",
      "Epoch 40/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8837 - val_loss: 0.4599 - val_accuracy: 0.8433\n",
      "Epoch 41/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8831 - val_loss: 0.4612 - val_accuracy: 0.8480\n",
      "Epoch 42/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8829 - val_loss: 0.4607 - val_accuracy: 0.8487\n",
      "Epoch 43/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8874 - val_loss: 0.4602 - val_accuracy: 0.8420\n",
      "Epoch 44/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8871 - val_loss: 0.4522 - val_accuracy: 0.8480\n",
      "Epoch 45/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8863 - val_loss: 0.4598 - val_accuracy: 0.8487\n",
      "Epoch 46/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8880 - val_loss: 0.4631 - val_accuracy: 0.8467\n",
      "Epoch 47/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8894 - val_loss: 0.4591 - val_accuracy: 0.8513\n",
      "Epoch 48/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8920 - val_loss: 0.4607 - val_accuracy: 0.8447\n",
      "Epoch 49/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8886 - val_loss: 0.4572 - val_accuracy: 0.8487\n",
      "Epoch 50/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8906 - val_loss: 0.4521 - val_accuracy: 0.8480\n",
      "Epoch 51/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8914 - val_loss: 0.4599 - val_accuracy: 0.8473\n",
      "Epoch 52/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8926 - val_loss: 0.4564 - val_accuracy: 0.8500\n",
      "Epoch 53/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8934 - val_loss: 0.4557 - val_accuracy: 0.8467\n",
      "Epoch 54/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8954 - val_loss: 0.4541 - val_accuracy: 0.8460\n",
      "Epoch 55/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8940 - val_loss: 0.4494 - val_accuracy: 0.8500\n",
      "Epoch 56/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8943 - val_loss: 0.4528 - val_accuracy: 0.8527\n",
      "Epoch 57/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8943 - val_loss: 0.4589 - val_accuracy: 0.8467\n",
      "Epoch 58/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8963 - val_loss: 0.4532 - val_accuracy: 0.8527\n",
      "Epoch 59/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3172 - accuracy: 0.8969 - val_loss: 0.4566 - val_accuracy: 0.8493\n",
      "Epoch 60/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8966 - val_loss: 0.4619 - val_accuracy: 0.8480\n",
      "Epoch 61/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8971 - val_loss: 0.4573 - val_accuracy: 0.8493\n",
      "Epoch 62/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.8980 - val_loss: 0.4573 - val_accuracy: 0.8540\n",
      "Epoch 63/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8989 - val_loss: 0.4476 - val_accuracy: 0.8513\n",
      "Epoch 64/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8966 - val_loss: 0.4516 - val_accuracy: 0.8540\n",
      "Epoch 65/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8960 - val_loss: 0.4545 - val_accuracy: 0.8533\n",
      "Epoch 66/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8977 - val_loss: 0.4557 - val_accuracy: 0.8367\n",
      "Epoch 67/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8969 - val_loss: 0.4527 - val_accuracy: 0.8493\n",
      "Epoch 68/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8977 - val_loss: 0.4628 - val_accuracy: 0.8400\n",
      "Epoch 69/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.9009 - val_loss: 0.4489 - val_accuracy: 0.8527\n",
      "Epoch 70/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.9017 - val_loss: 0.4502 - val_accuracy: 0.8560\n",
      "Epoch 71/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.9014 - val_loss: 0.4587 - val_accuracy: 0.8500\n",
      "Epoch 72/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.9009 - val_loss: 0.4539 - val_accuracy: 0.8480\n",
      "Epoch 73/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8991 - val_loss: 0.4621 - val_accuracy: 0.8400\n",
      "Epoch 74/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8997 - val_loss: 0.4612 - val_accuracy: 0.8500\n",
      "Epoch 75/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.9026 - val_loss: 0.4484 - val_accuracy: 0.8513\n",
      "Epoch 76/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.9034 - val_loss: 0.4516 - val_accuracy: 0.8453\n",
      "Epoch 77/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8997 - val_loss: 0.4599 - val_accuracy: 0.8487\n",
      "Epoch 78/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.9026 - val_loss: 0.4527 - val_accuracy: 0.8473\n",
      "Epoch 79/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9034 - val_loss: 0.4558 - val_accuracy: 0.8520\n",
      "Epoch 80/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.9040 - val_loss: 0.4534 - val_accuracy: 0.8440\n",
      "Epoch 81/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.9026 - val_loss: 0.4598 - val_accuracy: 0.8547\n",
      "Epoch 82/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.9060 - val_loss: 0.4470 - val_accuracy: 0.8460\n",
      "Epoch 83/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.9023 - val_loss: 0.4648 - val_accuracy: 0.8453\n",
      "Epoch 84/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.9089 - val_loss: 0.4599 - val_accuracy: 0.8480\n",
      "Epoch 85/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9037 - val_loss: 0.4491 - val_accuracy: 0.8447\n",
      "Epoch 86/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.9026 - val_loss: 0.4489 - val_accuracy: 0.8493\n",
      "Epoch 87/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.9057 - val_loss: 0.4503 - val_accuracy: 0.8440\n",
      "Epoch 88/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.9063 - val_loss: 0.4547 - val_accuracy: 0.8507\n",
      "Epoch 89/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.9071 - val_loss: 0.4566 - val_accuracy: 0.8487\n",
      "Epoch 90/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9037 - val_loss: 0.4477 - val_accuracy: 0.8540\n",
      "Epoch 91/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.9057 - val_loss: 0.4573 - val_accuracy: 0.8487\n",
      "Epoch 92/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.9086 - val_loss: 0.4517 - val_accuracy: 0.8453\n",
      "Epoch 93/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.9071 - val_loss: 0.4637 - val_accuracy: 0.8500\n",
      "Epoch 94/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.9069 - val_loss: 0.4517 - val_accuracy: 0.8500\n",
      "Epoch 95/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.9097 - val_loss: 0.4606 - val_accuracy: 0.8520\n",
      "Epoch 96/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.9066 - val_loss: 0.4580 - val_accuracy: 0.8480\n",
      "Epoch 97/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.9134 - val_loss: 0.4624 - val_accuracy: 0.8487\n",
      "Epoch 98/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9083 - val_loss: 0.4518 - val_accuracy: 0.8467\n",
      "Epoch 99/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.9057 - val_loss: 0.4523 - val_accuracy: 0.8460\n",
      "Epoch 100/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.9083 - val_loss: 0.4515 - val_accuracy: 0.8480\n",
      "Epoch 101/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.9106 - val_loss: 0.4607 - val_accuracy: 0.8540\n",
      "Epoch 102/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.9066 - val_loss: 0.4501 - val_accuracy: 0.8533\n",
      "Epoch 103/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9083 - val_loss: 0.4519 - val_accuracy: 0.8467\n",
      "Epoch 104/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.9069 - val_loss: 0.4529 - val_accuracy: 0.8427\n",
      "Epoch 105/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9094 - val_loss: 0.4578 - val_accuracy: 0.8447\n",
      "Epoch 106/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9123 - val_loss: 0.4473 - val_accuracy: 0.8527\n",
      "Epoch 107/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9131 - val_loss: 0.4574 - val_accuracy: 0.8427\n",
      "Epoch 108/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9114 - val_loss: 0.4618 - val_accuracy: 0.8513\n",
      "Epoch 109/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.9149 - val_loss: 0.4679 - val_accuracy: 0.8513\n",
      "Epoch 110/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9100 - val_loss: 0.4517 - val_accuracy: 0.8540\n",
      "Epoch 111/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.9114 - val_loss: 0.4549 - val_accuracy: 0.8540\n",
      "Epoch 112/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9143 - val_loss: 0.4562 - val_accuracy: 0.8500\n",
      "Epoch 113/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.9106 - val_loss: 0.4655 - val_accuracy: 0.8493\n",
      "Epoch 114/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9103 - val_loss: 0.4534 - val_accuracy: 0.8507\n",
      "Epoch 115/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9151 - val_loss: 0.4542 - val_accuracy: 0.8493\n",
      "Epoch 116/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.9111 - val_loss: 0.4523 - val_accuracy: 0.8487\n",
      "Epoch 117/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9129 - val_loss: 0.4641 - val_accuracy: 0.8507\n",
      "Epoch 118/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2597 - accuracy: 0.9140 - val_loss: 0.4600 - val_accuracy: 0.8520\n",
      "Epoch 119/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9137 - val_loss: 0.4685 - val_accuracy: 0.8520\n",
      "Epoch 120/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.9137 - val_loss: 0.4579 - val_accuracy: 0.8527\n",
      "Epoch 121/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9191 - val_loss: 0.4695 - val_accuracy: 0.8513\n",
      "Epoch 122/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.9120 - val_loss: 0.4653 - val_accuracy: 0.8433\n",
      "Epoch 123/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.9120 - val_loss: 0.4660 - val_accuracy: 0.8480\n",
      "Epoch 124/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.9149 - val_loss: 0.4622 - val_accuracy: 0.8513\n",
      "Epoch 125/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9140 - val_loss: 0.4615 - val_accuracy: 0.8520\n",
      "Epoch 126/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9174 - val_loss: 0.4587 - val_accuracy: 0.8547\n",
      "Epoch 127/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.9154 - val_loss: 0.4637 - val_accuracy: 0.8480\n",
      "Epoch 128/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.9143 - val_loss: 0.4595 - val_accuracy: 0.8487\n",
      "Epoch 129/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.9171 - val_loss: 0.4590 - val_accuracy: 0.8520\n",
      "Epoch 130/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9140 - val_loss: 0.4600 - val_accuracy: 0.8500\n",
      "Epoch 131/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9186 - val_loss: 0.4753 - val_accuracy: 0.8493\n",
      "Epoch 132/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.9180 - val_loss: 0.4752 - val_accuracy: 0.8467\n",
      "Epoch 133/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.9174 - val_loss: 0.4665 - val_accuracy: 0.8507\n",
      "Epoch 134/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.9186 - val_loss: 0.4657 - val_accuracy: 0.8520\n",
      "Epoch 135/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2496 - accuracy: 0.9151 - val_loss: 0.4655 - val_accuracy: 0.8453\n",
      "Epoch 136/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9169 - val_loss: 0.4729 - val_accuracy: 0.8540\n",
      "Epoch 137/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9203 - val_loss: 0.4686 - val_accuracy: 0.8560\n",
      "Epoch 138/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9163 - val_loss: 0.4594 - val_accuracy: 0.8547\n",
      "Epoch 139/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9177 - val_loss: 0.4666 - val_accuracy: 0.8553\n",
      "Epoch 140/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.9200 - val_loss: 0.4773 - val_accuracy: 0.8507\n",
      "Epoch 141/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9166 - val_loss: 0.4741 - val_accuracy: 0.8507\n",
      "Epoch 142/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9191 - val_loss: 0.4831 - val_accuracy: 0.8440\n",
      "Epoch 143/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9137 - val_loss: 0.4696 - val_accuracy: 0.8513\n",
      "Epoch 144/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9174 - val_loss: 0.4685 - val_accuracy: 0.8540\n",
      "Epoch 145/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9197 - val_loss: 0.4753 - val_accuracy: 0.8500\n",
      "Epoch 146/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9229 - val_loss: 0.4732 - val_accuracy: 0.8527\n",
      "Epoch 147/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9186 - val_loss: 0.4744 - val_accuracy: 0.8487\n",
      "Epoch 148/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9174 - val_loss: 0.4703 - val_accuracy: 0.8487\n",
      "Epoch 149/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9200 - val_loss: 0.4710 - val_accuracy: 0.8513\n",
      "Epoch 150/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9211 - val_loss: 0.4803 - val_accuracy: 0.8473\n",
      "Epoch 151/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2378 - accuracy: 0.9174 - val_loss: 0.4818 - val_accuracy: 0.8473\n",
      "Epoch 152/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9209 - val_loss: 0.4728 - val_accuracy: 0.8480\n",
      "Epoch 153/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9226 - val_loss: 0.4874 - val_accuracy: 0.8487\n",
      "Epoch 154/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.9223 - val_loss: 0.4788 - val_accuracy: 0.8507\n",
      "Epoch 155/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2378 - accuracy: 0.9211 - val_loss: 0.4715 - val_accuracy: 0.8480\n",
      "Epoch 156/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9214 - val_loss: 0.4694 - val_accuracy: 0.8460\n",
      "Epoch 157/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9174 - val_loss: 0.4789 - val_accuracy: 0.8513\n",
      "Epoch 158/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9240 - val_loss: 0.4873 - val_accuracy: 0.8493\n",
      "Epoch 159/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9249 - val_loss: 0.4699 - val_accuracy: 0.8507\n",
      "Epoch 160/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9203 - val_loss: 0.4702 - val_accuracy: 0.8500\n",
      "Epoch 161/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9229 - val_loss: 0.4783 - val_accuracy: 0.8453\n",
      "Epoch 162/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9226 - val_loss: 0.4841 - val_accuracy: 0.8433\n",
      "Epoch 163/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9246 - val_loss: 0.4819 - val_accuracy: 0.8467\n",
      "Epoch 164/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9217 - val_loss: 0.4831 - val_accuracy: 0.8493\n",
      "Epoch 165/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9223 - val_loss: 0.4815 - val_accuracy: 0.8527\n",
      "Epoch 166/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.9260 - val_loss: 0.5071 - val_accuracy: 0.8407\n",
      "Epoch 167/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9206 - val_loss: 0.4729 - val_accuracy: 0.8513\n",
      "Epoch 168/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.9254 - val_loss: 0.4766 - val_accuracy: 0.8547\n",
      "Epoch 169/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9206 - val_loss: 0.4829 - val_accuracy: 0.8520\n",
      "Epoch 170/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9274 - val_loss: 0.4823 - val_accuracy: 0.8453\n",
      "Epoch 171/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9234 - val_loss: 0.4939 - val_accuracy: 0.8420\n",
      "Epoch 172/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9240 - val_loss: 0.4800 - val_accuracy: 0.8567\n",
      "Epoch 173/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9220 - val_loss: 0.4853 - val_accuracy: 0.8473\n",
      "Epoch 174/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9234 - val_loss: 0.4893 - val_accuracy: 0.8480\n",
      "Epoch 175/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9240 - val_loss: 0.4820 - val_accuracy: 0.8507\n",
      "Epoch 176/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9257 - val_loss: 0.5165 - val_accuracy: 0.8420\n",
      "Epoch 177/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9234 - val_loss: 0.4884 - val_accuracy: 0.8480\n",
      "Epoch 178/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9254 - val_loss: 0.4832 - val_accuracy: 0.8433\n",
      "Epoch 179/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9240 - val_loss: 0.4965 - val_accuracy: 0.8473\n",
      "Epoch 180/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9226 - val_loss: 0.4952 - val_accuracy: 0.8487\n",
      "Epoch 181/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9246 - val_loss: 0.4878 - val_accuracy: 0.8467\n",
      "Epoch 182/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9234 - val_loss: 0.5016 - val_accuracy: 0.8487\n",
      "Epoch 183/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9209 - val_loss: 0.4869 - val_accuracy: 0.8527\n",
      "Epoch 184/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9289 - val_loss: 0.4813 - val_accuracy: 0.8547\n",
      "Epoch 185/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2213 - accuracy: 0.9289 - val_loss: 0.4948 - val_accuracy: 0.8507\n",
      "Epoch 186/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9223 - val_loss: 0.4858 - val_accuracy: 0.8493\n",
      "Epoch 187/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9269 - val_loss: 0.4970 - val_accuracy: 0.8520\n",
      "Epoch 188/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9243 - val_loss: 0.4916 - val_accuracy: 0.8527\n",
      "Epoch 189/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9280 - val_loss: 0.4913 - val_accuracy: 0.8473\n",
      "Epoch 190/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9314 - val_loss: 0.4957 - val_accuracy: 0.8500\n",
      "Epoch 191/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9286 - val_loss: 0.4905 - val_accuracy: 0.8447\n",
      "Epoch 192/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9214 - val_loss: 0.5015 - val_accuracy: 0.8420\n",
      "Epoch 193/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9234 - val_loss: 0.4988 - val_accuracy: 0.8500\n",
      "Epoch 194/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9274 - val_loss: 0.5011 - val_accuracy: 0.8467\n",
      "Epoch 195/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2182 - accuracy: 0.9294 - val_loss: 0.4885 - val_accuracy: 0.8493\n",
      "Epoch 196/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9277 - val_loss: 0.4895 - val_accuracy: 0.8513\n",
      "Epoch 197/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9300 - val_loss: 0.4902 - val_accuracy: 0.8547\n",
      "Epoch 198/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9303 - val_loss: 0.4920 - val_accuracy: 0.8487\n",
      "Epoch 199/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9303 - val_loss: 0.4893 - val_accuracy: 0.8487\n",
      "Epoch 200/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9309 - val_loss: 0.4971 - val_accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "keras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUkigFLQcAfL"
   },
   "source": [
    "##The Keras History Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJRNfnw6cGaT"
   },
   "source": [
    "The Keras history object has two main attributes- \n",
    "\n",
    "1)model \n",
    "\n",
    "2)history\n",
    "\n",
    "The model contains all information about the weights, inputs, activations and so on\n",
    "\n",
    "For example, to get the weights you would code:\n",
    "\n",
    "\n",
    "```\n",
    "keras_history.model.get_weights()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1AnMOSFhg20"
   },
   "source": [
    "The history is a dictionary containing training loss, training accuracy, validation loss, validation accuracy, etc. after each epoch. Each key contains an array of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vBUz_lDQh2y1",
    "outputId": "1231a5e6-58ca-4527-fa2f-000a6e648c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vq5LXY1iyfU"
   },
   "source": [
    "## Post training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwQOaGb2jCPX"
   },
   "source": [
    "Now we plot the training and test loss with each epoch. A key observation to make is that our model is overfitting- training accuracy is 94% while test accuracy is 87%. Notice how the training loss decreases while the test loss increases after a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "qMSG1xPvOAoI",
    "outputId": "9745afe7-ac6f-4cfa-e0af-4c3ab57f18eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f685300d5f8>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(keras_history.history['loss'], label='Training loss')\n",
    "plt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkvqbRxUjewo"
   },
   "source": [
    "#**IMPROVING YOUR NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h96i8MOKjlPb"
   },
   "source": [
    "Great! Now you know how to build a basic artificial neural network in Keras. However, ANN's are extremely prone to overfitting, so always keep a close tab on that. We will now see how various methods are deployed in keras to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KF4Hu4CVj9AK"
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0M6IXEmkBUt"
   },
   "source": [
    "You can apply an L1, L2 or both L1 and L2 regularizers to a layer using tf.keras.regularizers. The value of $\\lambda$ is specified as an argument of the class. For example\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1egJo7ylNrQ"
   },
   "source": [
    "Click [here](https://keras.io/api/layers/regularizers/#l1-class) to check out the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UXYeM_PlYvj"
   },
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwZfBan3lkgw"
   },
   "source": [
    "Nothing complicated here. Just do this:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jdr1DsTlw86"
   },
   "source": [
    "[Link for BatchNorm](https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa6aX97wmB2f"
   },
   "source": [
    "##Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jA6A5QBRmEsi"
   },
   "source": [
    "Nothing complicated here too. The probability of a neuron being \"dropped out\" is specified. Higher the value, the \"simpler\" the neural network is. But make sure not to specify too high a dropout rate else the model will undefit\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBalrMLBnXep"
   },
   "source": [
    "#**PUTTING IT ALL TOGETHER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXTTGh_5nZ_M"
   },
   "source": [
    "Here is all of the concepts we have just learnt. Please note that all the abovementioned techniques to prevent overfitting have been implemented below, to get an idea of how they are used in Keras. In reality, all these are not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "dQDQMwlBjd9N",
    "outputId": "ad70f7c3-9e8d-4080-d127-b5ec83f1fed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15)                60        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 1,055\n",
      "Trainable params: 965\n",
      "Non-trainable params: 90\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=10,))\n",
    "model.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.125))\n",
    "model.add(tf.keras.layers.Dense(units=15, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(units=5, activation='softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6b86CyX5qWB4",
    "outputId": "69944b9e-5f5f-4210-c3b0-cc229b6472d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.7178 - accuracy: 0.3863 - val_loss: 1.3750 - val_accuracy: 0.5247\n",
      "Epoch 2/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4278 - accuracy: 0.4823 - val_loss: 1.1908 - val_accuracy: 0.6013\n",
      "Epoch 3/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.2819 - accuracy: 0.5369 - val_loss: 1.0934 - val_accuracy: 0.6347\n",
      "Epoch 4/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.1795 - accuracy: 0.5894 - val_loss: 1.0117 - val_accuracy: 0.6720\n",
      "Epoch 5/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.1363 - accuracy: 0.6071 - val_loss: 0.9604 - val_accuracy: 0.6867\n",
      "Epoch 6/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.0804 - accuracy: 0.6286 - val_loss: 0.9037 - val_accuracy: 0.7080\n",
      "Epoch 7/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.0463 - accuracy: 0.6326 - val_loss: 0.8625 - val_accuracy: 0.7207\n",
      "Epoch 8/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9995 - accuracy: 0.6506 - val_loss: 0.8321 - val_accuracy: 0.7220\n",
      "Epoch 9/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9787 - accuracy: 0.6586 - val_loss: 0.7949 - val_accuracy: 0.7387\n",
      "Epoch 10/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9539 - accuracy: 0.6663 - val_loss: 0.7841 - val_accuracy: 0.7360\n",
      "Epoch 11/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.6720 - val_loss: 0.7500 - val_accuracy: 0.7487\n",
      "Epoch 12/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9117 - accuracy: 0.6866 - val_loss: 0.7403 - val_accuracy: 0.7507\n",
      "Epoch 13/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.9220 - accuracy: 0.6794 - val_loss: 0.7248 - val_accuracy: 0.7633\n",
      "Epoch 14/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8859 - accuracy: 0.6911 - val_loss: 0.7038 - val_accuracy: 0.7613\n",
      "Epoch 15/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8730 - accuracy: 0.6911 - val_loss: 0.6945 - val_accuracy: 0.7687\n",
      "Epoch 16/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8530 - accuracy: 0.6974 - val_loss: 0.6766 - val_accuracy: 0.7693\n",
      "Epoch 17/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8550 - accuracy: 0.7023 - val_loss: 0.6601 - val_accuracy: 0.7953\n",
      "Epoch 18/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8676 - accuracy: 0.6929 - val_loss: 0.6461 - val_accuracy: 0.7927\n",
      "Epoch 19/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.8251 - accuracy: 0.7117 - val_loss: 0.6339 - val_accuracy: 0.7893\n",
      "Epoch 20/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.8080 - accuracy: 0.7186 - val_loss: 0.6216 - val_accuracy: 0.7993\n",
      "Epoch 21/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.8264 - accuracy: 0.7114 - val_loss: 0.6200 - val_accuracy: 0.8013\n",
      "Epoch 22/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.8082 - accuracy: 0.7094 - val_loss: 0.6121 - val_accuracy: 0.7960\n",
      "Epoch 23/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.7891 - accuracy: 0.7243 - val_loss: 0.5973 - val_accuracy: 0.8053\n",
      "Epoch 24/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.7993 - accuracy: 0.7183 - val_loss: 0.5859 - val_accuracy: 0.8093\n",
      "Epoch 25/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7827 - accuracy: 0.7263 - val_loss: 0.5823 - val_accuracy: 0.8153\n",
      "Epoch 26/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7913 - accuracy: 0.7231 - val_loss: 0.5813 - val_accuracy: 0.7993\n",
      "Epoch 27/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7703 - accuracy: 0.7297 - val_loss: 0.5676 - val_accuracy: 0.8167\n",
      "Epoch 28/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7537 - accuracy: 0.7386 - val_loss: 0.5736 - val_accuracy: 0.8013\n",
      "Epoch 29/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7417 - accuracy: 0.7294 - val_loss: 0.5668 - val_accuracy: 0.8093\n",
      "Epoch 30/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7550 - accuracy: 0.7340 - val_loss: 0.5581 - val_accuracy: 0.8160\n",
      "Epoch 31/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7556 - accuracy: 0.7337 - val_loss: 0.5689 - val_accuracy: 0.8000\n",
      "Epoch 32/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7576 - accuracy: 0.7389 - val_loss: 0.5663 - val_accuracy: 0.8133\n",
      "Epoch 33/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7625 - accuracy: 0.7263 - val_loss: 0.5537 - val_accuracy: 0.8233\n",
      "Epoch 34/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7483 - accuracy: 0.7391 - val_loss: 0.5567 - val_accuracy: 0.8107\n",
      "Epoch 35/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7372 - accuracy: 0.7431 - val_loss: 0.5487 - val_accuracy: 0.8193\n",
      "Epoch 36/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7280 - accuracy: 0.7491 - val_loss: 0.5508 - val_accuracy: 0.8073\n",
      "Epoch 37/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7563 - accuracy: 0.7334 - val_loss: 0.5478 - val_accuracy: 0.8093\n",
      "Epoch 38/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7231 - accuracy: 0.7486 - val_loss: 0.5453 - val_accuracy: 0.8053\n",
      "Epoch 39/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7298 - accuracy: 0.7429 - val_loss: 0.5470 - val_accuracy: 0.8153\n",
      "Epoch 40/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.7156 - accuracy: 0.7449 - val_loss: 0.5475 - val_accuracy: 0.8100\n",
      "Epoch 41/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7233 - accuracy: 0.7443 - val_loss: 0.5444 - val_accuracy: 0.8133\n",
      "Epoch 42/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7073 - accuracy: 0.7486 - val_loss: 0.5431 - val_accuracy: 0.8180\n",
      "Epoch 43/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7261 - accuracy: 0.7420 - val_loss: 0.5393 - val_accuracy: 0.8227\n",
      "Epoch 44/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7377 - accuracy: 0.7389 - val_loss: 0.5454 - val_accuracy: 0.8093\n",
      "Epoch 45/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7221 - accuracy: 0.7454 - val_loss: 0.5264 - val_accuracy: 0.8233\n",
      "Epoch 46/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.7600 - val_loss: 0.5281 - val_accuracy: 0.8127\n",
      "Epoch 47/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7277 - accuracy: 0.7400 - val_loss: 0.5299 - val_accuracy: 0.8207\n",
      "Epoch 48/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7110 - accuracy: 0.7423 - val_loss: 0.5220 - val_accuracy: 0.8220\n",
      "Epoch 49/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.7529 - val_loss: 0.5185 - val_accuracy: 0.8267\n",
      "Epoch 50/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6979 - accuracy: 0.7511 - val_loss: 0.5213 - val_accuracy: 0.8213\n",
      "Epoch 51/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.7050 - accuracy: 0.7560 - val_loss: 0.5165 - val_accuracy: 0.8227\n",
      "Epoch 52/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.7070 - accuracy: 0.7551 - val_loss: 0.5179 - val_accuracy: 0.8220\n",
      "Epoch 53/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7041 - accuracy: 0.7583 - val_loss: 0.5205 - val_accuracy: 0.8233\n",
      "Epoch 54/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.7551 - val_loss: 0.5176 - val_accuracy: 0.8240\n",
      "Epoch 55/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.7614 - val_loss: 0.5187 - val_accuracy: 0.8233\n",
      "Epoch 56/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6997 - accuracy: 0.7597 - val_loss: 0.5111 - val_accuracy: 0.8273\n",
      "Epoch 57/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.7686 - val_loss: 0.5111 - val_accuracy: 0.8267\n",
      "Epoch 58/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.7569 - val_loss: 0.4999 - val_accuracy: 0.8273\n",
      "Epoch 59/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7019 - accuracy: 0.7566 - val_loss: 0.5175 - val_accuracy: 0.8307\n",
      "Epoch 60/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7093 - accuracy: 0.7500 - val_loss: 0.5125 - val_accuracy: 0.8333\n",
      "Epoch 61/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6995 - accuracy: 0.7571 - val_loss: 0.5217 - val_accuracy: 0.8167\n",
      "Epoch 62/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.7714 - val_loss: 0.5163 - val_accuracy: 0.8253\n",
      "Epoch 63/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6686 - accuracy: 0.7589 - val_loss: 0.5148 - val_accuracy: 0.8200\n",
      "Epoch 64/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.7534 - val_loss: 0.4992 - val_accuracy: 0.8333\n",
      "Epoch 65/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.7597 - val_loss: 0.5179 - val_accuracy: 0.8260\n",
      "Epoch 66/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.7583 - val_loss: 0.5100 - val_accuracy: 0.8280\n",
      "Epoch 67/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.7543 - val_loss: 0.5148 - val_accuracy: 0.8247\n",
      "Epoch 68/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.7637 - val_loss: 0.5053 - val_accuracy: 0.8287\n",
      "Epoch 69/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.7563 - val_loss: 0.4985 - val_accuracy: 0.8380\n",
      "Epoch 70/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6624 - accuracy: 0.7629 - val_loss: 0.5061 - val_accuracy: 0.8333\n",
      "Epoch 71/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.7689 - val_loss: 0.5124 - val_accuracy: 0.8280\n",
      "Epoch 72/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.7709 - val_loss: 0.5073 - val_accuracy: 0.8313\n",
      "Epoch 73/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.7531 - val_loss: 0.5088 - val_accuracy: 0.8307\n",
      "Epoch 74/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.7651 - val_loss: 0.5082 - val_accuracy: 0.8367\n",
      "Epoch 75/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.7657 - val_loss: 0.4915 - val_accuracy: 0.8380\n",
      "Epoch 76/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.7557 - val_loss: 0.4979 - val_accuracy: 0.8373\n",
      "Epoch 77/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.7697 - val_loss: 0.4933 - val_accuracy: 0.8307\n",
      "Epoch 78/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6744 - accuracy: 0.7569 - val_loss: 0.4925 - val_accuracy: 0.8360\n",
      "Epoch 79/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6749 - accuracy: 0.7657 - val_loss: 0.5023 - val_accuracy: 0.8347\n",
      "Epoch 80/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.7654 - val_loss: 0.4909 - val_accuracy: 0.8373\n",
      "Epoch 81/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.7717 - val_loss: 0.5042 - val_accuracy: 0.8327\n",
      "Epoch 82/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6745 - accuracy: 0.7671 - val_loss: 0.4908 - val_accuracy: 0.8347\n",
      "Epoch 83/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.7611 - val_loss: 0.4979 - val_accuracy: 0.8287\n",
      "Epoch 84/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.7723 - val_loss: 0.4944 - val_accuracy: 0.8347\n",
      "Epoch 85/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.7700 - val_loss: 0.4935 - val_accuracy: 0.8340\n",
      "Epoch 86/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.7537 - val_loss: 0.4921 - val_accuracy: 0.8367\n",
      "Epoch 87/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.7694 - val_loss: 0.4979 - val_accuracy: 0.8293\n",
      "Epoch 88/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6503 - accuracy: 0.7749 - val_loss: 0.4988 - val_accuracy: 0.8293\n",
      "Epoch 89/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.7809 - val_loss: 0.4942 - val_accuracy: 0.8273\n",
      "Epoch 90/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.7654 - val_loss: 0.4956 - val_accuracy: 0.8333\n",
      "Epoch 91/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6635 - accuracy: 0.7583 - val_loss: 0.4941 - val_accuracy: 0.8293\n",
      "Epoch 92/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.7697 - val_loss: 0.4900 - val_accuracy: 0.8360\n",
      "Epoch 93/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.7680 - val_loss: 0.4996 - val_accuracy: 0.8433\n",
      "Epoch 94/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.7671 - val_loss: 0.5004 - val_accuracy: 0.8300\n",
      "Epoch 95/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.7751 - val_loss: 0.4835 - val_accuracy: 0.8420\n",
      "Epoch 96/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6518 - accuracy: 0.7743 - val_loss: 0.4797 - val_accuracy: 0.8373\n",
      "Epoch 97/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.7597 - val_loss: 0.4838 - val_accuracy: 0.8387\n",
      "Epoch 98/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.7589 - val_loss: 0.4925 - val_accuracy: 0.8360\n",
      "Epoch 99/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.7674 - val_loss: 0.4897 - val_accuracy: 0.8327\n",
      "Epoch 100/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.7743 - val_loss: 0.4986 - val_accuracy: 0.8240\n",
      "Epoch 101/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.7671 - val_loss: 0.4847 - val_accuracy: 0.8393\n",
      "Epoch 102/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.7597 - val_loss: 0.4873 - val_accuracy: 0.8460\n",
      "Epoch 103/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.7746 - val_loss: 0.4955 - val_accuracy: 0.8433\n",
      "Epoch 104/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.7709 - val_loss: 0.4809 - val_accuracy: 0.8367\n",
      "Epoch 105/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.7629 - val_loss: 0.4772 - val_accuracy: 0.8507\n",
      "Epoch 106/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6661 - accuracy: 0.7666 - val_loss: 0.4973 - val_accuracy: 0.8247\n",
      "Epoch 107/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.7657 - val_loss: 0.4763 - val_accuracy: 0.8400\n",
      "Epoch 108/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6589 - accuracy: 0.7683 - val_loss: 0.4841 - val_accuracy: 0.8347\n",
      "Epoch 109/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.7640 - val_loss: 0.4966 - val_accuracy: 0.8347\n",
      "Epoch 110/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.7689 - val_loss: 0.4969 - val_accuracy: 0.8320\n",
      "Epoch 111/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.7611 - val_loss: 0.4796 - val_accuracy: 0.8393\n",
      "Epoch 112/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.7649 - val_loss: 0.4912 - val_accuracy: 0.8300\n",
      "Epoch 113/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.7766 - val_loss: 0.4845 - val_accuracy: 0.8493\n",
      "Epoch 114/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.7737 - val_loss: 0.4802 - val_accuracy: 0.8440\n",
      "Epoch 115/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.7740 - val_loss: 0.4867 - val_accuracy: 0.8347\n",
      "Epoch 116/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.7791 - val_loss: 0.4777 - val_accuracy: 0.8460\n",
      "Epoch 117/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6484 - accuracy: 0.7726 - val_loss: 0.4748 - val_accuracy: 0.8333\n",
      "Epoch 118/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.7651 - val_loss: 0.4722 - val_accuracy: 0.8460\n",
      "Epoch 119/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.7620 - val_loss: 0.4783 - val_accuracy: 0.8327\n",
      "Epoch 120/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6541 - accuracy: 0.7629 - val_loss: 0.4657 - val_accuracy: 0.8427\n",
      "Epoch 121/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6275 - accuracy: 0.7820 - val_loss: 0.4788 - val_accuracy: 0.8367\n",
      "Epoch 122/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.7689 - val_loss: 0.4912 - val_accuracy: 0.8393\n",
      "Epoch 123/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.7600 - val_loss: 0.4786 - val_accuracy: 0.8393\n",
      "Epoch 124/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6428 - accuracy: 0.7751 - val_loss: 0.4816 - val_accuracy: 0.8380\n",
      "Epoch 125/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6538 - accuracy: 0.7626 - val_loss: 0.4973 - val_accuracy: 0.8347\n",
      "Epoch 126/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6467 - accuracy: 0.7737 - val_loss: 0.4767 - val_accuracy: 0.8367\n",
      "Epoch 127/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.7646 - val_loss: 0.4748 - val_accuracy: 0.8427\n",
      "Epoch 128/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.7743 - val_loss: 0.4786 - val_accuracy: 0.8327\n",
      "Epoch 129/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6581 - accuracy: 0.7649 - val_loss: 0.4923 - val_accuracy: 0.8273\n",
      "Epoch 130/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.7631 - val_loss: 0.4824 - val_accuracy: 0.8340\n",
      "Epoch 131/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.7731 - val_loss: 0.4785 - val_accuracy: 0.8347\n",
      "Epoch 132/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.7700 - val_loss: 0.4871 - val_accuracy: 0.8287\n",
      "Epoch 133/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.7683 - val_loss: 0.4803 - val_accuracy: 0.8347\n",
      "Epoch 134/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.7697 - val_loss: 0.4776 - val_accuracy: 0.8340\n",
      "Epoch 135/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.7711 - val_loss: 0.4881 - val_accuracy: 0.8267\n",
      "Epoch 136/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.7637 - val_loss: 0.4796 - val_accuracy: 0.8293\n",
      "Epoch 137/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.7774 - val_loss: 0.4822 - val_accuracy: 0.8353\n",
      "Epoch 138/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.7654 - val_loss: 0.4763 - val_accuracy: 0.8387\n",
      "Epoch 139/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6335 - accuracy: 0.7769 - val_loss: 0.4755 - val_accuracy: 0.8340\n",
      "Epoch 140/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6381 - accuracy: 0.7740 - val_loss: 0.4826 - val_accuracy: 0.8353\n",
      "Epoch 141/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.7669 - val_loss: 0.4790 - val_accuracy: 0.8353\n",
      "Epoch 142/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.7791 - val_loss: 0.4745 - val_accuracy: 0.8407\n",
      "Epoch 143/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6745 - accuracy: 0.7600 - val_loss: 0.4769 - val_accuracy: 0.8420\n",
      "Epoch 144/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6558 - accuracy: 0.7726 - val_loss: 0.4901 - val_accuracy: 0.8320\n",
      "Epoch 145/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6460 - accuracy: 0.7749 - val_loss: 0.4717 - val_accuracy: 0.8433\n",
      "Epoch 146/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.7689 - val_loss: 0.4822 - val_accuracy: 0.8253\n",
      "Epoch 147/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.7789 - val_loss: 0.4814 - val_accuracy: 0.8327\n",
      "Epoch 148/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6594 - accuracy: 0.7671 - val_loss: 0.4825 - val_accuracy: 0.8380\n",
      "Epoch 149/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.7703 - val_loss: 0.4767 - val_accuracy: 0.8320\n",
      "Epoch 150/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.7711 - val_loss: 0.4784 - val_accuracy: 0.8393\n",
      "Epoch 151/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.7786 - val_loss: 0.4841 - val_accuracy: 0.8347\n",
      "Epoch 152/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.7709 - val_loss: 0.4850 - val_accuracy: 0.8347\n",
      "Epoch 153/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.7757 - val_loss: 0.4862 - val_accuracy: 0.8340\n",
      "Epoch 154/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6506 - accuracy: 0.7751 - val_loss: 0.4818 - val_accuracy: 0.8360\n",
      "Epoch 155/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.7751 - val_loss: 0.4874 - val_accuracy: 0.8293\n",
      "Epoch 156/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.7706 - val_loss: 0.4682 - val_accuracy: 0.8433\n",
      "Epoch 157/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7611 - val_loss: 0.4817 - val_accuracy: 0.8333\n",
      "Epoch 158/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.7729 - val_loss: 0.4780 - val_accuracy: 0.8320\n",
      "Epoch 159/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.7851 - val_loss: 0.4814 - val_accuracy: 0.8400\n",
      "Epoch 160/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.7697 - val_loss: 0.4668 - val_accuracy: 0.8373\n",
      "Epoch 161/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.7769 - val_loss: 0.4699 - val_accuracy: 0.8387\n",
      "Epoch 162/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.7820 - val_loss: 0.4702 - val_accuracy: 0.8413\n",
      "Epoch 163/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.7677 - val_loss: 0.4777 - val_accuracy: 0.8420\n",
      "Epoch 164/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.7749 - val_loss: 0.4707 - val_accuracy: 0.8413\n",
      "Epoch 165/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.7717 - val_loss: 0.4701 - val_accuracy: 0.8393\n",
      "Epoch 166/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.7751 - val_loss: 0.4602 - val_accuracy: 0.8420\n",
      "Epoch 167/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.7654 - val_loss: 0.4688 - val_accuracy: 0.8427\n",
      "Epoch 168/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.7763 - val_loss: 0.4609 - val_accuracy: 0.8460\n",
      "Epoch 169/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.7654 - val_loss: 0.4651 - val_accuracy: 0.8460\n",
      "Epoch 170/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.7717 - val_loss: 0.4863 - val_accuracy: 0.8320\n",
      "Epoch 171/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.7714 - val_loss: 0.4727 - val_accuracy: 0.8440\n",
      "Epoch 172/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6543 - accuracy: 0.7749 - val_loss: 0.4879 - val_accuracy: 0.8360\n",
      "Epoch 173/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.7714 - val_loss: 0.4864 - val_accuracy: 0.8393\n",
      "Epoch 174/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.7766 - val_loss: 0.4966 - val_accuracy: 0.8187\n",
      "Epoch 175/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.7691 - val_loss: 0.4770 - val_accuracy: 0.8253\n",
      "Epoch 176/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6224 - accuracy: 0.7774 - val_loss: 0.4697 - val_accuracy: 0.8400\n",
      "Epoch 177/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6636 - accuracy: 0.7617 - val_loss: 0.4805 - val_accuracy: 0.8373\n",
      "Epoch 178/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.7723 - val_loss: 0.4861 - val_accuracy: 0.8340\n",
      "Epoch 179/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.7863 - val_loss: 0.4684 - val_accuracy: 0.8460\n",
      "Epoch 180/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.7723 - val_loss: 0.4808 - val_accuracy: 0.8353\n",
      "Epoch 181/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.7783 - val_loss: 0.4682 - val_accuracy: 0.8373\n",
      "Epoch 182/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.7797 - val_loss: 0.4752 - val_accuracy: 0.8300\n",
      "Epoch 183/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.7766 - val_loss: 0.4715 - val_accuracy: 0.8340\n",
      "Epoch 184/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.7680 - val_loss: 0.4769 - val_accuracy: 0.8313\n",
      "Epoch 185/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7737 - val_loss: 0.4627 - val_accuracy: 0.8433\n",
      "Epoch 186/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.7774 - val_loss: 0.4608 - val_accuracy: 0.8527\n",
      "Epoch 187/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.7791 - val_loss: 0.4892 - val_accuracy: 0.8320\n",
      "Epoch 188/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.7729 - val_loss: 0.4601 - val_accuracy: 0.8453\n",
      "Epoch 189/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.7771 - val_loss: 0.4728 - val_accuracy: 0.8420\n",
      "Epoch 190/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.7689 - val_loss: 0.4766 - val_accuracy: 0.8347\n",
      "Epoch 191/200\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.7846 - val_loss: 0.4696 - val_accuracy: 0.8340\n",
      "Epoch 192/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.7831 - val_loss: 0.4805 - val_accuracy: 0.8347\n",
      "Epoch 193/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.7877 - val_loss: 0.4847 - val_accuracy: 0.8333\n",
      "Epoch 194/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.7769 - val_loss: 0.4734 - val_accuracy: 0.8360\n",
      "Epoch 195/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.7723 - val_loss: 0.4892 - val_accuracy: 0.8340\n",
      "Epoch 196/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.7811 - val_loss: 0.4782 - val_accuracy: 0.8347\n",
      "Epoch 197/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.7706 - val_loss: 0.4719 - val_accuracy: 0.8460\n",
      "Epoch 198/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.7834 - val_loss: 0.4736 - val_accuracy: 0.8327\n",
      "Epoch 199/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.7657 - val_loss: 0.4797 - val_accuracy: 0.8313\n",
      "Epoch 200/200\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.7746 - val_loss: 0.4863 - val_accuracy: 0.8320\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "keras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "1BpghfAMqcEW",
    "outputId": "fc570e9a-d05f-447c-a698-3c80d651104d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f68530b0ef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(keras_history.history['loss'], label='Training loss')\n",
    "plt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1B1b4YcoMoY"
   },
   "source": [
    "**We now urge you to tweak various hyperparamers, such as the learning rate,  dropout rate, regularization parameter, batch size, number of layers, number of neurons, etc. to get an idea of how the model performs with such changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNvrOIEypT9v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biJIIaHWpUCw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHgbYB8QpUk5"
   },
   "source": [
    "Awesome! Hope you've gotten the gist of how to use Keras and how simple and straightforward it is to use."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KerasIntro",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
