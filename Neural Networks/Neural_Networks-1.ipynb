{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LeIph-SRhGct"
   },
   "source": [
    "# Neural Networks 1 - Introduction and Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chlC8WrmhGcu"
   },
   "source": [
    "## 1. Introduction and Intuition\n",
    "\n",
    "The first thing that you will see in any machine learning tutorial is the following image, and told that it is a mystic black box that, when you give some input, it makes some predictions or has an output. \n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/neural_network_w_matrices.png\" width=75%>\n",
    "So lets work on demystifying this blackbox, and breaking this image into pieces and understanding what each component means, and by the end you must have a pretty good grasp of how a neural network works. Let's start with the most fundamental component of this blackbox and build our way up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES-OuuEfhGcv"
   },
   "source": [
    "### 1.1 Neurons\n",
    "\n",
    "A Neuron can be thought of as a single unit of logistic Regression. It takes in an array of inputs, computes it's dot product with the weight vector of that neuron and add the bais, and finally apply an activation function to it, to return an output. \n",
    "\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/neuron.png\" width=50%>\n",
    "\n",
    "Now, the inputs to each neuron can be the Data you feed to the network or the output from other neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WahdmQahGcw"
   },
   "source": [
    "### 1.2 Layers\n",
    "\n",
    "These neurons, \"stacked in a column\" make a layer of the neural network. Stacked in a column implies that the weight matrices, and bias of each of the neurons are stacked one on op of the other. This is what gives us the weight matrix of that layer, denoted by $W$.\n",
    "\n",
    "Say the size of the input vector to the layer, and hence EACH NEURON is $n_0$, and the number of neurons in this layer is $n_1$\n",
    "1. Shape of $w$, the weight matrix of a single neuron: $(1, n_0)$\n",
    "\n",
    "So, if each of these $n_1$ row vectors are stacked on top of each other, we get a matrix $W$. This matrix has $n_1$ rows, and $n_0$ rows. Hence, \n",
    "2. The shape of $W$, the weight matrix of the layer is $(n_1, n_0)$\n",
    "\n",
    "Similarly, whereas the bias for the logistic regression classifier was a scalar, the bias of the layer is a column vector ($n_1$ biases stacked on top of each other) of shape $(n_1, 1)$. Note that, the bias will be represented here by $b$ as well. The reason for this will be clear soon.\n",
    "\n",
    "The equation of propagation of each layer is an intuitive extension of the one we looked at for Logistic Regression. \n",
    "\n",
    "Expressing the input of the layer by $a_0$ containing $n_0$ features, and output of the layer by $a_1$:\n",
    "\n",
    "$$ a_1 = W \\cdot a_0 + b $$\n",
    "\n",
    "From the rules of dot product, it is clear that $a_1$ has shape $(n_1, 1)$\n",
    "\n",
    "> Note: From now, each neuron will also be referred to as a unit of a layer. For example, a layer might have 3 units, meaning it has 3 neurons.\n",
    "\n",
    "> You might even see people call the input vector as a layer as well. This leads to a more fundamental discussion of whether the weights are what make a layer, or the features. If that does not make sense, then don't worry. The concepts will be explained in detail, just remember that there is a reason why the input vector to a Neural Network is called an \"Input Layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Po4u3VYUhGcw"
   },
   "source": [
    "### 1.3 The Neural Network as a Classfier\n",
    "\n",
    "A Neural Network is an ordered arrangement of several of these layers with different Number of units, activation functions, etc. The number of layers in a Neural Network, and the number of units in almost all layers is the decision of the programmer.\n",
    "\n",
    "> Note: All the various numbers that can be tweaked by the programmer, to make a model better are called **hyperparameters** (as compared to the parameters, the weights and the biases). Some examples of hyperparameters that you have come across till now are learning rate, $\\alpha$, the number of Layers in a Network and the number of units in a layer. \n",
    "\n",
    "The job of a Neural Network is to compute the approximate probabilities of a given input belonging to a particular class. If there are 5 classes that it can belong to (eg: \"cat\", \"dog\", \"bat\", \"ant\", and others) then the final output of the netwrosk might look something like this:\n",
    "\n",
    "Index | Label | Probability\n",
    "---|---|---|\n",
    "0 | \"cat\" | 0.12\n",
    "1 | \"dog\" | 0.04\n",
    "2 | \"bat\" | 0.7\n",
    "3 | \"ant\" | 0.07\n",
    "4 | \"others\" | 0.7\n",
    "\n",
    "From this we can infer that, $P(\\text{image is of a cat} | \\text{image} = x) = 0.12 $\n",
    "\n",
    "> Going with the Logistic Regression Analogy, the Sigmoid Activation Function won't suffice, since it returns just a single Scalar whereas we need a vector of probabilities. For a multiclass classifier we will use the natural extension of the Sigmoid Function, or the Softmax function. There are several other activation functions as well, all of which we will look at soon.\n",
    "\n",
    "This kind of indexing is called one hot encoding where the Labels are converted into a column vector, where the value of class which it belongs to is 1, rest are 0. Indicating that the probability of the input belonging to that class is 1, and rest is 0. The one-hot encoding for this Input might look like $[[0.],[0.], [1.], [0.], [0.]]$ indicating that the input actually belongs to class given by index 2, or \"bat\"\n",
    "\n",
    "\n",
    "The trivial neural network would be a binary logistic regression classifier, with just a single layer (the output layer) and two possible classes.\n",
    "\n",
    "Taking one more step ahead, lets add one more layer, between the input Layer, and the output Layer.\n",
    "\n",
    "The Input Vector $x$, will be fed into the layer H. Let's call it's output $a_h$.These output of the Hidden Layer, can also be considered a feature vector, and will be used as input to the next layer. \n",
    "\n",
    "> Now, since this $a_h$ is \"hidden\" from the  user, it is called a hidden layer\n",
    "\n",
    "This $a_h$ will be fed into the Output Layer, which gives you the final output or the probabilities of the the $x$ belonging to the different classes.\n",
    "\n",
    "This, finally leads us to the picture we saw earlier: \n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/neural_network_w_matrices.png\">\n",
    "\n",
    "Now, the Neural Network described above has a single hidden layer. However, most Networks will have several Hidden layers with various sizes.\n",
    "\n",
    "Idiomatically, the Number of layers in a Neural Network is given by the Number of Hidden Layers + the Output Layer, and is denoted by $L$. The Input Layer is, by extension the $0^{th}$ layer.\n",
    "\n",
    "Hence, the Neural Network we described is a 2 Layer Neural Network, or a Neural Network with 1 Hidden Layer.\n",
    "\n",
    "> The number of layers, the number of units per layer, etc describe the **Architecture of the Neural Network**. Other items which we have not yet covered that also describe the architecture include Activation Function of each layer, or the type of layer among several others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZWXUY5mhGcx"
   },
   "source": [
    "## 2. Forward Propagation in a 2 Layer Network\n",
    "\n",
    "\n",
    "Now that we know what a Neural Network does, let us look at how it does it. \n",
    "\n",
    "Assume that we have a 2 Layer Model, which takes an $n_x$ dimensional feature vector and there are 2 classes, ie binary classification.\n",
    "\n",
    "If the number of units in each layer is given by  $n^{[l]}$, where $l$ denoted the layer number, then\n",
    "\n",
    "1. Input Layer has, $n^{[0]} = n_x$ units\n",
    "2. Hidden Layer has $n^{[1]}$ units, \n",
    "3. Output Layer has $n^{[2]} = 1$ units\n",
    "\n",
    "Let the input vector be $x$ and have the shape $(n_x, 1)$ or $(n^{[0]}, 1)$. \n",
    "\n",
    "The input vector is then dotted with the Weight Matrix of the hidden Layer, and Bias added to get the Linear Output of the Hidden Layer. After which we apply the Activation function - Sigmoid. In equation form, \n",
    "\n",
    "$$ z^{[1]} = W^{[1]} \\cdot x + b^{[1]} $$\n",
    "$$ a^{[1]} = S(z^{[1]}) $$\n",
    "\n",
    "Where, the superscipt [1] denotes the first layer. \n",
    "\n",
    "> Since the Hidden Layer, or Layer 1 has $n^{[1]}$ units, it must have $n^{[1]}$ rows as well. And for the dot priduct to make sense, it must have $n^{[0]}$ columns. Therfore, $W^{[1]}$ has the shape $(n^{[1]}, n^{[0]})$ \n",
    "> $b^{[1]}$ has the shape $(n^{[1]}, 1)$\n",
    "\n",
    "\n",
    "Now, we use the $a^{[1]}$ as input to the 2nd Layer, the Output Layer.\n",
    "Using a similar algorithm, we get:\n",
    "\n",
    "$$ z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]} $$\n",
    "$$ \\hat y = a^{[2]} = S(z^{[2]}) $$\n",
    "\n",
    "> As we did for the Hidden Layer, we can derive the shape of $W^{[2]}$ as $(n^{[2]}, n^{[1]})$ and $b^{[2]}$ has the shape $(n^{[2]}, 1)$\n",
    "\n",
    "\n",
    "These steps, complete the Process of forward Propagation in the network. For this week, we will be looking into 2 Layer Networks only. We will generalise these results to L Layer Network in a Subsequent Week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2Q5ixYBhGcy"
   },
   "source": [
    "### 2.1 Initialization of the Parameters\n",
    "\n",
    "In the Logistic Regression Classifier, we initialized the weights with a zero vector, and the bias with zero and over time optimized their value. However, the same thing can not be apllied to a Neural Network. \n",
    "\n",
    "tldr; Zero or Constant Initialization fails to break symetry of neurons\n",
    "\n",
    "Let us assume that we have a 2 Layer Network, with weights and biases of all layers are initialized to a constant Number, say $c$. \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/agyRr.png\">\n",
    "\n",
    "Now, notice that each unit in the Hidden Layer will get the exact same signal. (Each Neuron does computation on the entire Input Vector)\n",
    "\n",
    "i.e., the Each Hidden Unit will do the following computation:\n",
    "\n",
    "$$ z^{[1]}_j = W^{[1]}_{ij} \\cdot x + b^{[1]}_{i} $$\n",
    "$$ a^{[1]}_j = S(z^{[1]}_j) $$ \n",
    "\n",
    "$ W^{[1]}_{ij} $ denotes the $ j^{th} $ element in the weight vector of the $ i^{th} $ Neuron in the 1st Layer (ie Hidden Layer)\n",
    "\n",
    "So, since $ W^{[1]}_{ij} = c,  \\forall i \\in [1, n^{[1]}], \\forall j \\in [1, n^{[0]}] $\n",
    "So, each Unit is doing the EXACT SAME CALCULATION. Eg, if c = 1 then each Neuron is just calculating the sum of the input vector. And so all the neurons, output the same signal.\n",
    "\n",
    "This renders your entire Neural Network model, with several thousand connections or \"synapses\", with a glorified Logistic Regresssion Model.\n",
    "\n",
    "> For an amazing visualization of this exact phenomena, visit [Initializing Neural Networks by deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/)\n",
    "\n",
    "So, in essence we will always use random initialization for the Weights. Biases can still be initialized as zero, since W has been initialized randomly, and symtetry already broken.\n",
    "\n",
    "> Other more sofisticated methods of Initialization exist as well, eg: Xavier Initialization. However, they are out of scope of this notebook\n",
    "\n",
    "> Think about how backpropagation would work; In backprop you need to now update each weight matrix and each bias vector. Try to come up with the expressions on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIUxkQe-hGcy"
   },
   "source": [
    "### 2.2 Activation Functions \n",
    "\n",
    "Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Activation functions give neural networks their power — allowing them to model complex non-linear relationships. By modifying inputs with non-linear functions neural networks can model highly complex relationships between features. \n",
    "\n",
    "Till now the only activation function you have used is the sigmoid function. However, there are several other functions which give much better accuracy when used in the hidden Layer(s). \n",
    "\n",
    "Activation Functions Typically have the following Properties:\n",
    "\n",
    "1. Non-Linear: In linear regression we’re limited to a prediction equation that looks like a straight line. This is nice for simple datasets with a one-to-one relationship between inputs and outputs, but what if the patterns in our dataset were non-linear? (e.g. $x^2$, $sin$, $log$). To model these relationships we need a non-linear prediction equation. Activation functions provide this non-linearity.\n",
    "\n",
    "2. Continuously differentiable: To improve our model with gradient descent, we need our output to have a nice slope so we can compute error derivatives with respect to weights. \n",
    "\n",
    "3. Fixed Range: Activation functions typically squash the input data into a narrow range that makes training the model more stable and efficient.\n",
    "\n",
    "Let us look at a few more Activation Functions in the next section\n",
    "> Exercise: Try to write the code for each of the following activation functions an theeir derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgVS4QBqhGcz"
   },
   "source": [
    "### 2.2.1 Rectified Linear Unit (ReLU)\n",
    "\n",
    "The Rectified Linear Unit is a very simple Activation Functions. For all positive Real Numbers it is the number itself, and for everything else it is 0.\n",
    "\n",
    "In Mathematical Form,\n",
    "\n",
    "Function | Derivative\n",
    "---|---|\n",
    "$$\n",
    "\\begin{equation}\n",
    "    R(z) =\n",
    "    \\begin{cases}\n",
    "        z \\text{, if } z \\gt 0\\\\\n",
    "        0 \\text{, if } z \\le 0\\\\\n",
    "     \\end{cases}\n",
    "\\end{equation}\n",
    "$$ | $$\n",
    "\\begin{equation}\n",
    "    R'(z) =\n",
    "    \\begin{cases}\n",
    "        1 \\text{, if } z \\gt 0\\\\\n",
    "        0 \\text{, if } z \\lt 0\\\\\n",
    "     \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu.png\" width=250px> | <img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu_prime.png\" width=250px>\n",
    "\n",
    "\n",
    "1. ReLU has been shown to perfrom very well for Deep Neural Networks.\n",
    "2. It is way less computationally cheaper than Sigmoid or other activation functions we'll soon see. \n",
    "3. Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n",
    "3. ReLU can be used only in the Hidden Layers of a neural Network. \n",
    "4. ReLU is that it has a range of $[0, \\infty)$. This means that it can blow up the activation of a layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qg4_VunghGc0"
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    if(z>0):\n",
    "        return z\n",
    "    else :\n",
    "        return 0\n",
    "#ReLU(-72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOJBATrDhGc4"
   },
   "source": [
    "### 2.2.2 Leaky ReLU\n",
    "\n",
    "LeakyRelu is a variant of ReLU. Instead of being 0 when z<0, a leaky ReLU allows a small, non-zero, constant gradient α (Normally, α=0.01). However, the consistency of the benefit across tasks is presently unclear.\n",
    "\n",
    "In Mathematical Form,\n",
    "\n",
    "Function | Derivative\n",
    "---|---|\n",
    "$$\n",
    "\\begin{equation}\n",
    "    R(z) =\n",
    "    \\begin{cases}\n",
    "        z \\text{, if } z \\gt 0\\\\\n",
    "        \\alpha z \\text{, if } z \\le 0\\\\\n",
    "     \\end{cases}\n",
    "\\end{equation}\n",
    "$$ | $$\n",
    "\\begin{equation}\n",
    "    R'(z) =\n",
    "    \\begin{cases}\n",
    "        1 \\text{, if } z \\gt 0\\\\\n",
    "        \\alpha \\text{, if } z \\lt 0\\\\\n",
    "     \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu.png\" width=250px> | <img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu_prime.png\" width=250px>\n",
    "\n",
    "\n",
    "1. Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).\n",
    "2. As it possess linearity, it can’t be used for the complex Classification. It lags behind the Sigmoid and Tanh for some of the use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlwZO44ohGc4"
   },
   "outputs": [],
   "source": [
    "def leaky_ReLU(z , alpha = 0.01):\n",
    "    if(z>0):\n",
    "        return z\n",
    "    else:\n",
    "        return alpha*z\n",
    "#leaky_ReLU(-72 , 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO_olNBihGc8"
   },
   "source": [
    "### 2.2.3 Hyperbolic Tangent\n",
    "\n",
    "Function | Derivative\n",
    "---|---|\n",
    "$$ g(z) = tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$ | $$ g'(z) = 1 - tanh^2(z) $$\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh.png\" width=250px> | <img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh_prime.png\" width=250px>\n",
    "\n",
    "1. tanh squashes a real-valued number to the range [-1, 1]\n",
    "2. It’s non-linear. But unlike Sigmoid, its output is zero-centered\n",
    "3. tanh gradient is stronger than the sigmoid (the derivatives are steeper)\n",
    "Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity for non-output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET4dh7hLhGc9"
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return ((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\n",
    "#tanh(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syekgky_hGdA"
   },
   "source": [
    "### 2.2.4 Softmax\n",
    "\n",
    "Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs. \n",
    "\n",
    "It is used for multiclass classification and we will look into it in greater detail when covering deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftMaxReg(A):\n",
    "    return A/np.sum(A)\n",
    "#SoftMaxReg([1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvdOlcwKhGdB"
   },
   "source": [
    "## Glossary of Notation\n",
    "\n",
    "### Sizes\n",
    "* $m$: Number of trining Examples\n",
    "* $n_x$: Input Size\n",
    "* $n_y$: Output Size or Number of classes (The only exception is that when doing binary classification although there are 2 classes, $n_y = 1$\n",
    "* L: Number of Layers in the Neural Network\n",
    "* $n^{[l]}$: Number of units in the layer $l, \\forall l \\in [0, L]$\n",
    "\n",
    " - $n^{[0]} = n_x$\n",
    " - $n^{[0]} = n_y$\n",
    "\n",
    "### Objects\n",
    "#### Inputs\n",
    "* $X \\in \\mathbb{R}^{n_x \\times m}$ is the input matrix\n",
    "* $x^{(i)} \\in \\mathbb{R}^{n_x}$ is the $i^{th}$ example represented as a column vector\n",
    "\n",
    "#### Outputs\n",
    "* $Y \\in \\mathbb{R}^{n_y \\times m}$ is the label matrix\n",
    "* $y^{(i)} \\in \\mathbb{R}^{n_y}$ is the output lable $i^{th}$ example\n",
    "* $\\hat Y \\in \\mathbb{R}^{n_y \\times m}$ is the predicted Output Vector\n",
    "\n",
    "#### Features\n",
    "* $A^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ is the feature vector of the $l^{th}$ layer\n",
    "* $a^{[l](i)} \\in \\mathbb{R}^{n_n^{[l]} \\times m}$ is the feature vector of the $l^{th}$ layer, and  $i^{th}$ example\n",
    "\n",
    " - $A^{[0]} = X$\n",
    " - $A^{[L]} = \\hat Y$\n",
    "\n",
    "#### Parameters\n",
    "* $ W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}} $ is the weight Matrix of the $ l^{th} $ layer\n",
    "* $ b^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ is the Bias Vector of the $ l^{th} $ layer\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6_Neural_Networks-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
