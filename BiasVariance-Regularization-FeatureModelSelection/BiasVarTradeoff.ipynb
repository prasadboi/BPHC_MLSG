{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPROVING MACHINE LEARNING MODELS - 1 : Bias and Variance\n",
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "So far we have seen 3 different models under Supervised Learning: Linear Regression, Logistic Regression and Neural Networks. While training you have seen that your model seems to be performing considerably well. However, when you test it on unseen data the accuracy seems to fall. Sometimes your model may not perform well even on the train set. In this topic, we'll see why these might happen and some techniques that might help overcome these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important quality that a Machine Learning model should have is <b>generalization</b>. Generalization refers to property of a model to perform well on previously unseen data. So to measure how well our model generalizes, we use a method called cross validation. In this method, instead of training on the entire data that we have, we don't train on some part of the data and use it to evaluate generalization of our model.\n",
    "\n",
    "There are two types of cross validation:\n",
    "1. Train/test split\n",
    "2. K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split\n",
    "\n",
    "This is a method that you are already familiar with. We divide the dataset into two parts: Training and Testing. We use the training set to train, and testing set to tune and evaluate the model. In this method, sometimes the model may overfit the test set since the hyperparameters have been chosen to perform best on the test set. Because of this, when the model is actually deployed and starts getting real world examples, it may not perform well.\n",
    "\n",
    "To avoid this, you can split the dataset into three parts: Train, Validation and Test. The Validation set is used to tune your hyperparameters and select the best model and then the Test set is used to report the final accuracy. But most of the time, splitting into Train and test should work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross validation\n",
    "\n",
    "In this method:\n",
    "<ol>\n",
    "    <li>The dataset is shuffled and divided into K equal parts.</li>\n",
    "    <li>For each part, use this as the validation set and remaining for training and note the accuracies. In total, you'll have to train and validate K times.</li>\n",
    "    <li>The average training accuracy and validation accuracy over all K iterations is considered.</li>\n",
    "</ol>\n",
    "\n",
    "This method is generally better when compared to the previous one. However this is computationally more expensive. Generally K is chosen as 5 or 10.\n",
    "\n",
    "Even here, you can keep part of the data for exclusive testing and apply K-fold cross validation to the remaining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Note:</b><br>\n",
    "    <ul>\n",
    "    <li>If there is no seperate test set to report test accuracy (i.e, data is divided into 2 parts only), validation accuracy can be reported.</li>\n",
    "        <li>The training set is significantly larger than the other sets.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation:\n",
    "<ul>\n",
    "<li>Scikit Learn provides helpful functions for Cross Validation. Check out <a href = \"https://scikit-learn.org/stable/modules/cross_validation.html\">this</a> for the cross validation methods in scikit learn.</li>\n",
    "<li>We've also seen that Cross validation is also used for hyperparameter tuning. Check <a href = \"https://scikit-learn.org/stable/modules/grid_search.html\">this</a> out to know about the methods that scikit learn provides for systematic hyperparameter tuning.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "The two major problems that your model may generally face are the problems of <b>overfitting</b> or <b>underfitting</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the image given below. The training data points are scattered in the forms of crosses. We have tried to fit a regression model for the given data. From the scattered points, it's clear that the data follows a quadratic relationship.<br><br>\n",
    "<center><img src = \"https://image.ibb.co/bDA9TR/overfitting2.png\" width = '600'></center>\n",
    "\n",
    "> Image credits: Andrew Ng's Introduction to ML course on coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Underfitting</u></b>:<br>\n",
    "In the left image, the model is too simple (less number of parameters) to capture the quadratic relationship ad hence <b>underfits</b> the data. This condition is also known as <b>high bias</b>. Other observations in this case are that, the the model is fitting poorly on the train data and hence train error is high. If you imagine adding new data points (following quadratic relationship) the model will perform poorly on them too. Hence test error will be high as well. So, <b>in a high bias problem, the model performs poorly on <u>both training and test sets</u>.</b>\n",
    "<br><br>\n",
    "<b><u>Overfitting</u></b>:<br>\n",
    "In the right image, the model is too complicated (high number of parameters) and again fails to capture the quadratic relationship and manages to learn all the unnecessary noise in the data. This condition is also known as <b>high variance</b>. Other observations in this case are that, the model is fitting very well on the train data and hence train error will be low. However, if you imagine adding new data points (following quadratic relationship) the model will perform poorly on them. Hence test error will be high. So, <b>In a high variance problem, the model <u>performs well on the training set</u> but <u>performs poorly on the test set</u>.</b>\n",
    "<br><br>\n",
    "In general, underfitting is easy to detect as your training error itself will be high. On the other hand, overfitting is harder to detect as your model will perform well on the training set, but poorly on the validation or test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Variance Tradeoff\n",
    "\n",
    "Bias refers to how close the predicted value is to the true value. The farther the predicted value from the true value, the higher the bias. An underfitting model has high bias as it predicts values far from the true values.\n",
    "\n",
    "Variance indicates the spread of the data. The higher the spread, the higher the variance. An overfitting model learns all the noise in the data and hence predicts data points with a large spread.\n",
    "\n",
    "Consider the bulls-eye diagram given below.<br><br>\n",
    "<center><img src = \"https://miro.medium.com/max/936/1*xwtSpR_zg7j7zusa4IDHNQ.png\" width = \"400\"></center>\n",
    "\n",
    "> Image credits: <a href = \"https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\">Link</a>\n",
    "\n",
    "<br>\n",
    "As we can see, the best model has both low bias and low variance. Complicated models tend to have high variance and simpler models tend to have high bias. The process of simultaneously minimizing both bias and variance is called bias variance trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common fixes for high bias\n",
    "\n",
    "<ul>\n",
    "    <li>Increasing complexity of the model (Increasing degree of polynomial in regression, increasing number of hidden layers in Neural Networks etc.).</li>\n",
    "    <li>Adding new features.</li>\n",
    "    <li>Decreasing Regularization (which you will see in the next part).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common fixes for high variance\n",
    "\n",
    "<ul>\n",
    "    <li>Decreasing complexity of the model.</li>\n",
    "    <li>Removing less significant features.</li>\n",
    "    <li>Increasing Regularization (which you will see in the next part).</li>\n",
    "    <li>Getting more training data (As amount of training data increases the model will be forced to generalize to all points instead of overfitting to less points).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve\n",
    "\n",
    "Learning Curve is a plot of your evaluation metric (e.g, error or accuracy) against number of training examples (m). It is a diagnostic tool to identify overfitting and underfitting.\n",
    "\n",
    "To plot a learning curve, take 1 training example from the training set, train your model and compute training and test set errors. Next take 2 training examples from training set, train your model and compute training and test set errors. Repeat the process until you train for all m examples and calculate errors. Now plot the errors on y-axis and number of training examples on x-axis.\n",
    "\n",
    "Note that if you train on k examples, you compute training error on those k examples while you compute test error on the entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting\n",
    "\n",
    "Let's consider a Regression model where the model is too simple and will tend to underfit on the entire training set.\n",
    "\n",
    "If you start training, the training error will be 0 initially and gradually increase to reach a high value. Why does this happen? As you know we can always fit an exact straight line (linear model) when we have 2 data points. We can always fit an exact parabola (quadratic model) when we have 3 data points. Hence, initially training error will be 0. However as we increase the number of training examples the error will gradually increase, and in case of a simple model, by the time we train over all the training examples, we'll have a high training error.\n",
    "\n",
    "On the other hand, the validation error will be extremely high initially (due to a bad fit and hence less generalization) and decrease gradually as the fit gets better and better and reach a saturation value around the same as training error.\n",
    "\n",
    "This is what a learning curve would look like in an underfitting case.\n",
    "\n",
    "<center><img src = \"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bpAOvt9uEeaQlg5FcsXQDA_ecad653e01ee824b231ff8b5df7208d9_2-am.png?expiry=1591833600000&hmac=qNnis-K7iWHLPlmyGQ5PoiyExdXuF17CN88hLfhyUsg\" width = \"400\"></center>\n",
    "\n",
    "> Image credits: Andrew Ng's Introduction to ML course on coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "Let's consider a Regression model where the model is complicated and will tend to overfit the entire training set.\n",
    "\n",
    "Similar to the previous case, the training will be 0 initially and then slowly increase. However since the model is complicated, the training error will saturate at a low value unlike the previous case.\n",
    "\n",
    "On the other hand, the validation error will be extremely high initially like the previous case and slowly decrease to stop at a high error value like the previous case. There will be a gap between the training and validation error curves.\n",
    "\n",
    "This is what a learning curve would look like in an overfitting case.\n",
    "\n",
    "<center><img src = \"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/vqlG7t9uEeaizBK307J26A_3e3e9f42b5e3ce9e3466a0416c4368ee_ITu3antfEeam4BLcQYZr8Q_37fe6be97e7b0740d1871ba99d4c2ed9_300px-Learning1.png?expiry=1591833600000&hmac=VIh8ZGZtrOclzTiRnlGMdPsF5HiG4SjLjGFaEmHf8ag\" width = \"400\"></center>\n",
    "\n",
    "> Image credits: Andrew Ng's Introduction to ML course on coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "<ul>\n",
    "    <li>In case of high bias (underfitting), the <b>training error increases to a high value</b> and <b>test error decreases to around the same value</b>. <b><u>Low gap between train and test error curves</u></b>.</li>\n",
    "    <li>In case of high variance (overfitting), the <b>training error increases to a low value</b> and <b>test error decreases to a high value</b>. <b><u>High gap between train and test error curves</u></b>.</li>\n",
    "    <li><b><u>In case of high bias, getting more training examples will not help. In case of high variance, getting more training examples will help.</u></b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Note</b>:<br>\n",
    "    If accuracy is plotted instead of error, the curve will look like a reflection of this with respect to the x-axis\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Scikit Learn provides methods to easily plot learning curves. Check <a href = \"https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\">this</a> out for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curve\n",
    "\n",
    "With large number of training examples, it is computationally expensive to plot. Validation curve can overcome this issue.\n",
    "\n",
    "In validation curves, the evaluation metric (e.g, losses, error or accuracy) is plotted against various hyperparameters (e.g, number of epochs, degree of polynomial in regression) for the training and validation/test sets.\n",
    "\n",
    "For example if we plot losses against model complexity:\n",
    "<ul>\n",
    "    <li><b>Low gap</b> between the two curves with <b>high training error</b> indicates region of <b>high bias</b>.</li>\n",
    "    <li><b>Low gap</b> between the two curves with <b>low training error</b> indicates region of <b>good fit</b>.</li>\n",
    "    <li><b>High gap</b> between the two curves with <b>low training error</b> indicates region of <b>high variance</b>.</li>\n",
    "</ul>\n",
    "<br><br>\n",
    "<center><img src = \"https://n2value.com/blog/wp-content/uploads/2014/03/Overfitting-.jpg\" width = \"600\"></center>\n",
    "\n",
    "> Image credits: <a href = \"https://n2value.com/blog/where-im-going-with-this-blog/\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Scikit Learn provides methods to easily plot validation curves. Check <a href = \"https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve\">this</a> out for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
