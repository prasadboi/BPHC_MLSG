{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPROVING MACHINE LEARNING MODELS - 3: Reducing Variance in Neural Networks\n",
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dropout regularization\n",
    "\n",
    "You have already seen L1 and L2 Regularization, which  penalises large values in the Weight Matrix, and effectively compresses the model.\n",
    "\n",
    "Dropout has a completely different approach to reaching the optimal. As the name suggests, at each iteration you randomly \"dropout\", or remove nodes from each layer at each iteraation. So effectively, training a much smaller model each time. \n",
    "\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/regularization-dropout.PNG\">\n",
    "\n",
    "> image from https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "During testing, the entire fully trained model is used, and no nodes dropped, so we have a much better trained model. \n",
    "\n",
    "We will be using a method called Inverted Dropout to achieve dropout Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implementation of Dropout: Inverted Dropout\n",
    "\n",
    "Consider the $l^{th}$ layer $L^{[l]}$ in the training sequence. The task is to effectively Zero out random features or activities in this Layer. For this, we will use a **Dropout** vector, denoted by $d^{[l]}$ \n",
    "\n",
    "The vector $d^{[l]}$ consists of 0s and 1s, with the probability of any element being 1, given by $p_{keep}^{[l]}$ and having the same shape as $a^{[l]}$\n",
    "\n",
    "> Exercise: Create the required dropout vector, with the shape same as $a^{[l]}$, and with probability of any element being 0.7\n",
    "Hint: Use np.random.rand and the astype('float') method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_l = np.random.randn(7, 6) * 0.01\n",
    "keep_prob = 0.7\n",
    "\n",
    "d_l = np.ones((7,6))\n",
    "d_1 = d_1*keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, $d^{[l]}$ vector is multiplied with $a^{[l]}$, elementwise. Now, to maintain the same 'size' of the feature vector, we scale up $a^{[l]}$, by dividing by the $p_{keep}^{[l]}$\n",
    "\n",
    "> Exercise: Perform the dropout on the feature vector a_l, and scale it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_l *= None\n",
    "a_l /= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Only use dropout when training your model. Using dropout during testing/validation/production phase adds noise into the Results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Dropout to Effectively reduce Overfitting\n",
    "\n",
    "A rule of thumb, is that to reduce overfitting, the layers with the more nodes have a lower $p_{keep}^{[l]}$. Consider you have a model with the following architecture (3, 8, 9, 7, 2, 1). \n",
    "Then you can have:\n",
    "1. $p_{keep}^{[1]} = 1$\n",
    "1. $p_{keep}^{[2]} = 0.7$\n",
    "1. $p_{keep}^{[3]} = 0.5$\n",
    "1. $p_{keep}^{[4]} = 0.7$\n",
    "1. $p_{keep}^{[5]} = 1$\n",
    "1. $p_{keep}^{[6]} = 1$\n",
    "\n",
    "The intuition behind this can be that decreasing the probability of keeping a neuron reduces the chances of overfitting the huge matrix. \n",
    "\n",
    "### 1.3 Disadvantages\n",
    "1. Dropout fucks with cost functions. (Why?) So calculate cost keeping $p_{keep}^{[l]} = 1 \\forall l \\in [1, L]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Normalization\n",
    "\n",
    "Batch Normalization is the process of Normalizing each feature of the batch, to have Mean 0 and Variance 1 and optionally changing the Mean and Variance to some other value. \n",
    "Consider the Layer, $l$:\n",
    "\n",
    "1. Calculating the Mean of the m training samples of each feature:\n",
    "\n",
    "$$ \\mu^{[l]} = \\frac{1}{m} \\sum_{i = 1}^m Z^{[l](i)} $$\n",
    "\n",
    "1. Calculating the Variance of the m training samples of each feature:\n",
    "\n",
    "$$ (\\sigma^{[l]})^2 = \\frac{1}{m} \\sum_{i = 1}^m (Z^{[l](i)} -  \\mu^{[l]}) $$\n",
    "\n",
    "3. Normalize each feature of the training sample\n",
    "\n",
    "$$ Z^{[l](i)}_{norm} = \\frac{Z^{[l](i)} - \\mu^{[l](i)}}{ \\sigma^{[l](i)} + \\epsilon } $$\n",
    "\n",
    "4. Modify the mean and Variance of each feature (Optional, can sometimes lead to better results)\n",
    "\n",
    "$$ \\tilde Z^{[l](i)} = \\gamma^{[l]} * Z^{[l](i)}_{norm} + \\beta^{[l]} $$\n",
    "\n",
    "5. Proceed with the next equations just as you would, but replace $ Z^{[l](i)} $ with $ \\tilde Z^{[l](i)} $, eg:\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(\\tilde Z^{[l]}) $$\n",
    "\n",
    "6. $\\gamma^{[l]}$ and $\\beta^{[l]} $ are trainable parameters, similar to $W^{[l]}$ and $b^{[l]}$. So optimize them as well. For example if you use Gradient Descent:\n",
    "\n",
    "$$ \\beta ^{[l]} = \\beta^{[l]} - \\alpha \\frac{\\partial J}{\\partial \\beta^{[l]}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about Batch Normalization\n",
    "\n",
    "1. Consider the primary forward pass equaion,\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "Calculating the mean, always results in cancelling out the addition of the bias vector. Which means that it's presence is not required and can be effectively removed. \n",
    "\n",
    "2. $ Z^{[l]} $, $ b^{[l]} $, $ \\beta^{[l]} $ and $\\gamma^{[l]}$ have the same shape: $(n^{[l]}, 1)$ due to the element wise products and additions being done. \n",
    "\n",
    "2. Batch Normalization has slight Regularization effect, but it must not be used intentionally for that. The source of the regularizing effect is the noise in $\\mu$ and $\\sigma$\n",
    "\n",
    "3. Batch Normalization makes a model more robust to be able to deal with covariate shift. Please check online for what covariate shift means. \n",
    "> A quick explanation can be found in Andrew NG's Improving Deep Learning Models Course on Coursera.\n",
    "\n",
    "4. When performing training, keep an exponentially weighted moving average of the  $\\mu^{[l]}$ and $\\sigma^{[l]}$ of training samples that you encounter,  $\\mu^{[l]}_{avg}$ and $\\sigma^{[l]}_{avg}$. These $ \\mu^{[l]}_{avg}$ and $\\sigma^{[l]}_{avg} $ will be used in testing/validation/production. \n",
    "\n",
    "> For Batch Gradient Descent, $\\mu^{[l]}_{avg} = \\mu^{[l]}$ and $\\sigma^{[l]}_{avg} = \\sigma^{[l]}$ due to the same dataaset being passed each epoch.\n",
    "\n",
    "> Exponentially weighted moving average can be calculated using the following formula:\n",
    "$$ V_t = \\beta V_{t-1} + (1 - \\beta)\\theta_t $$ where, \n",
    "1. $V_t$ is the moving average at the $ t^{th} $ iteration\n",
    "2. $V_0 = 0$\n",
    "2. $ \\theta_t $ is the next value to be added\n",
    "3. $ \\beta \\in [0, 1) $ dictates the effect of the current value, $\\theta_t$ on the moving average,  $V_t$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}