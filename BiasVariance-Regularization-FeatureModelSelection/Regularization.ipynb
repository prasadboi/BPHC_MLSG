{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypIrOjHTMLjp"
   },
   "source": [
    "# IMPROVING MACHINE LEARNING MODELS - 2 : Regularization\n",
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEIt-2ffMPVh"
   },
   "source": [
    "One of the simplest ways of preventing overfitting is the use of regularization.\n",
    "\n",
    "Overfitting happens when a model learns the very specific pattern and noise from the training data to such an extent that it negatively impacts our model’s ability to generalize from our training data to new (“unseen”) data. By noise, we mean the irrelevant information or randomness in a dataset.\n",
    "\n",
    "Preventing overfitting is very necessary to improve the performance of our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjUSTjKeMpTL"
   },
   "source": [
    "## What is regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gs8KjdgMuFa"
   },
   "source": [
    "In general, regularization means to make things regular or acceptable. This is exactly why we use it for applied machine learning. In the context of machine learning, regularization is the process which regularizes or shrinks the coefficients towards zero. In simple words, regularization discourages learning a more complex or flexible model, to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfIoqghKOVfm"
   },
   "source": [
    "## Simple Intuition behind Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdngIl1xOZFn"
   },
   "source": [
    "Let's go back to our linear regression module in which we tried to predict the cost of a house. The predicted cost of our house might look like a function of X1 and X2 and X3 as:\n",
    "\n",
    "$\\hat{y} = 20843.6764X_{1} + 1893.12765X_{2} + 97.12131X_{3}$\n",
    "\n",
    "and so on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObpMLHuQQBHA"
   },
   "source": [
    "Looks very complex and dangerous isn’t it?\n",
    "The function has trained itself to get the correct target values for all the noise induced data points and thus has failed to predict the correct pattern. This function may give very less error for training set but will give huge errors in predicting the correct target values for test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Euaq51_6QQmd"
   },
   "source": [
    "## How Regularization is Implemented in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADfSBjRZQY9-"
   },
   "source": [
    "How do we implement regularization in ML? We usually add a penalty term in the cost function, be it linear regression,logistic regression or neural networks. This penalty term is used so that the weights do not become too large or complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hvD643YQ87A"
   },
   "source": [
    "Consider the loss function for neural networks which is \n",
    "\n",
    "$$J(Y, \\hat Y) = -\\frac{1}{m} \\sum\\limits_{i = 1}^m y^{(i)} \\log (\\hat y^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8hA6e8sS4Bq"
   },
   "source": [
    "The new cost function with regularization is given by \n",
    "\n",
    "$$J = -\\frac{1}{m} [\\sum\\limits_{i = 1}^m y^{(i)} \\log (\\hat y^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat y^{(i)})] + \\frac{\\lambda}{2}\\sum\\limits_{l=1}^ L\\sum\\limits_{j=1}^{n_{l}}|(w_{j}^{[l]})| ^{k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77H6sRiXV0KD"
   },
   "source": [
    "where,\n",
    "\n",
    "$n_{l}$ = number of neurons in layer l,\n",
    "\n",
    "L = number of layers\n",
    "\n",
    "$w_{j}^{[l]}$ = the 'jth' weight in layer l\n",
    "\n",
    "\n",
    "You can basically think of it as multiplying the sum of the kth power of the weights by a value lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ki26ZXiU4z9"
   },
   "source": [
    "*The degree or value of k is usually set to 1 or 2*\n",
    "\n",
    "1) When n=1, we call it L1 regularization\n",
    "\n",
    "2) When n=2, we call it L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mI3I61yxVWkJ"
   },
   "source": [
    "In linear, logistic regression , and neural networks, we use L2 regularization as it tends to penalize the weights more, with equal importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhbTt4ArdLO0"
   },
   "source": [
    "># L1 vs L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GloBm87HdQH9"
   },
   "source": [
    "**L1 regularization:** Take a look at the L1 Equation. It has the effect of pushing W towards 0, leading to sparsity.\n",
    "\n",
    "This is of course is pointless in a 1-variable linear regression model, but will prove its prowess to ‘remove’ useless variables in multivariate regression models. You can also think of L1 as reducing the number of features in the model altogether. Here is an arbitrary example of L1 trying to ‘push’ some variables in a multivariate linear regression model:\n",
    "\n",
    "So how does pushing w towards 0 help in overfitting in L1 regularisation? As mentioned above, as w goes to 0, we are reducing the number of features by reducing the variable importance. Therefore, L1 leads to a form of feature selection. This in turn reduces the model complexity, making our model simpler. A simpler model can reduce the chances of overfitting.\n",
    "\n",
    "\n",
    "**L2 regularization:** Take a look at the L2 Equation. It is more unlikely in L2 for weights to near zero as there is a square element. However, for the same value, weights are more penalized in L2 than L1. In addition, L2 maintains all the features while penalizing their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN3w0e0UVhnQ"
   },
   "source": [
    "## How it Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nq7IFEq7VkQq"
   },
   "source": [
    "We now have a new hyperparameter $\\lambda$. Lambda controls how much the weights must be penalized. \n",
    "\n",
    "Let us say $\\lambda$ is a relatively small value, like 0.001. We have to minimize the cost function J, but since the weights are multiplied by a relatively small number such as 0.001, they are still given freedom to take larger values.\n",
    "\n",
    "Now assume $\\lambda$ to be large, like 10 (In fact, this is too big a value for $\\lambda$!) Each weight is multiplied by 10 times but the cost has to minimized so the weights are forced to take smaller and simpler values. This way, overfitting is prevented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzupA-OvanyG"
   },
   "source": [
    "## Built-in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o7TY5fFas8W"
   },
   "source": [
    "Scikit learn provides both L1, L2 and other regularization techniques. Check [this link](https://scikit-learn.org/stable/modules/linear_model.html) out for linear models and their various regularization techniques in scikit learn. Even in deep learning frameworks such as Keras, there are many regularization techniques to prevent overfitting in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzHCnsPNXYbJ"
   },
   "source": [
    "## Side Note: Gradient Computation and Effect on Training Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8GEyck7Xfdg"
   },
   "source": [
    "During backprop/ gradient descent, when regularization is used, the value of gradients are different. We suggest you to calculate the gradients with regularization (they're not that hard :) )\n",
    "\n",
    "Effect on Training Accuracy: When regularization is used, since weights are penalized, the training accuracy decreases. However, this isn't a primary concern as with the proper choice of lambda, a balance can be achieved. Moreover, the test accuracy is a true metric of a model's robustness as it is a representation of the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUxs-Ws2YkYA"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii9KGCQGYvFY"
   },
   "source": [
    "Regularization is a simple yet powerfult technique to the overfitting problem. It is used in almost every single regression technique or neural network. From now on you are advised to use regularization in every model you create."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7S0zIKvYJp/NuvdCMtIxy",
   "collapsed_sections": [],
   "name": "Bias and Variance 2: Regularization",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
